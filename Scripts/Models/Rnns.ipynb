{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Deep Learning Introduction***\n",
    "\n",
    "*Kurze erklärung:*\n",
    "\n",
    "\n",
    "***Deep Learning bezeichnet eine Methode des maschinellen Lernens, die künstliche neuronale Netze mit.***\n",
    "\n",
    "***zahlreichen Zwischenschichten zwischen Eingabeschicht und Ausgabeschicht einsetzt und dadurch eine.***\n",
    " \n",
    "***umfangreiche innere Struktur herausbildet. Es ist eine spezielle Methode der Informationsverarbeitung.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Grundlegende ideen bei einem Neural Network***\n",
    "\n",
    "Ein Deep Neural Network (DNN) ist eine leistungsstarke Technik des maschinellen Lernens, die in verschiedenen Bereichen Anwendung findet. Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Muster in Daten zu erkennen und Vorhersagen oder Klassifikationen basierend auf diesen Mustern zu treffen. (Siehe auch weiter oben)\n",
    "\n",
    "\n",
    " Hier sind einige der Schlüsselaspekte, die die Bedeutung von DNNs verdeutlichen:\n",
    "\n",
    "\n",
    "- 1. Mustererkennung: DNNs sind in der Lage, Muster in großen und komplexen Datensätzen zu erkennen (die nicht Linear sind), die von Menschen schwer oder unmöglich zu identifizieren sind. Dies macht sie besonders wertvoll in Anwendungen wie Bilderkennung, Spracherkennung und Natural Language Processing (NLP).\n",
    "\n",
    "\n",
    "- 2. Klassifikation und Vorhersage: DNNs ermöglichen die Klassifikation von Daten in verschiedene Kategorien oder die Vorhersage zukünftiger Ereignisse. Dies wird in Anwendungen wie Spam-Erkennung, Gesichtserkennung, medizinischer Diagnose und Finanzprognosen genutzt.\n",
    "\n",
    "\n",
    "- 3. Skalierbarkeit: DNNs können große Mengen an Daten verarbeiten und sind in der Lage, Modelle mit Tausenden oder Millionen von Parametern zu trainieren. Dies ermöglicht die Bewältigung komplexer Aufgaben und die Verarbeitung großer Datensätze.\n",
    "\n",
    "\n",
    "- 4. Automatisierung: DNNs können automatisch und kontinuierlich lernen, was bedeutet, dass sie sich an neue Daten und Veränderungen in der Umgebung anpassen können, ohne dass eine manuelle Anpassung erforderlich ist.\n",
    "\n",
    "\n",
    "- 5. Vielseitigkeit: DNNs finden in verschiedenen Bereichen Anwendung, darunter Bildverarbeitung, Sprachverarbeitung, autonome Fahrzeuge, medizinische Diagnostik, Empfehlungssysteme und vieles mehr. Sie sind eine Kernkomponente von Künstlicher Intelligenz (KI) und Deep Learning.\n",
    "\n",
    "\n",
    "- 6. Forschung und Innovation: DNNs haben zu bedeutenden Fortschritten in der KI-Forschung geführt und neue Möglichkeiten für innovative Anwendungen eröffnet. Sie sind ein Motor für die Entwicklung neuer Technologien und Lösungen.\n",
    "\n",
    "\n",
    "Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Aufgaben zu bewältigen und menschenähnliche Leistungen bei der Verarbeitung von Informationen zu erzielen. Dies hat Auswirkungen auf viele Branchen und trägt zur Transformation von Geschäftsmodellen und zur Lösung komplexer Herausforderungen bei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full,y_train_full), (X_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), dtype('uint8'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Labeling der Klassen**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstellung des Modells:**\n",
    "- Mit ein Input Layer zwei Hidden Layers und ein Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300,activation=\"relu\"),\n",
    "    keras.layers.Dense(100,activation=\"relu\"),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Erklärung der Schichten***\n",
    "1. Flatten (Input layer): nimmt die Eingabe und macht daraus ein 1D Array\n",
    "2. Dense: 300 Neuronen, mit ReLU Aktivierungsfunktion\n",
    "    Die erste Sicht hat bspweise \n",
    "    \n",
    "        28 x 28 = 784 \n",
    "\n",
    "        784 x 300 + 300 Bias-Terme = 235.500\n",
    "                                \n",
    "- was sich zu 235.500 Parameter ausfummiert. Jedoch Risiko overfitting.\n",
    "3. Dense: 100 Neuronen, mit ReLU Aktivierungsfunktion\n",
    "4. Dense (Output Layer): 10 Neuronen, mit Softmax Aktivierungsfunktion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste mit den sichten eines Modells erhalten, um eine davon über ihren Index anzusprechen, oder Sie gehen über den Namen vor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x175bdb4f0>,\n",
       " <keras.layers.core.dense.Dense at 0x175bb5a90>,\n",
       " <keras.layers.core.dense.Dense at 0x175bb5e20>,\n",
       " <keras.layers.core.dense.Dense at 0x175bdb3d0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(hidden1.name) is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights() # get_weights() and set_weights() are used to get and set the weights of a layer\n",
    "print(biases)\n",
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Es ist gut das alle biases auf 0 gesetzt sind. Da wir das so möchten um die Symeterie zu erhalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Das Model compilen und trainieren***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparse_categorical_crossentropy wird verwendet, weil wir spärliche Beschriftungen haben (d.h. für jede Instanz gibt es nur einen Zielklassenindex, in diesem Fall von 0 bis 9), und die Klassen sind exklusiv.\n",
    "\n",
    "- Hätten wir stattdessen eine Zielwahrscheinlichkeit pro Klasse für jede Instanz \n",
    "                \n",
    "        (wie z.B. One-Hot-Vektoren, z.B. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], um Klasse 3 zu repräsentieren), \n",
    "\n",
    "- dann müssten wir stattdessen den categorical_crossentropy loss verwenden.\n",
    "\n",
    "- Wenn wir eine binäre Klassifizierung durchführen (mit einem oder mehreren binären Labels), dann würden wir in der Ausgabeschicht anstelle der Softmax-Aktivierungsfunktion die sigmoide (d.h. logistische) Aktivierungsfunktion verwenden und den binary_crossentropy loss einsetzen.\n",
    "\n",
    "- Wenn wir regressieren würden, dann würden wir keine Aktivierungsfunktion in der Ausgabeschicht verwenden, und wir würden den Verlust mean_squared_error verwenden.\n",
    "\n",
    "- Der Optimierer ist der Algorithmus, der zur Aktualisierung der Gewichte des Modells verwendet wird. Der gebräuchlichste Optimierer ist der Stochastische Gradientenabstieg (SGD), aber es gibt noch viele andere, und einige von ihnen sind viel schneller als SGD. Einige Optimierer sind auch in der Lage, die Lernrate während des Trainings automatisch zu verringern, so dass Sie sie nicht manuell einstellen müssen. Dies ist der Fall beim Adam-Optimierer, der sehr beliebt ist.\n",
    "\n",
    "- Das Argument metrics wird verwendet, um die zu berechnenden Metriken anzugeben. In diesem Fall geht es nur um die Genauigkeit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 17:03:09.507621: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 2s 899us/step - loss: 0.7274 - accuracy: 0.7609 - val_loss: 0.5141 - val_accuracy: 0.8272\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4904 - accuracy: 0.8283 - val_loss: 0.4435 - val_accuracy: 0.8476\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 975us/step - loss: 0.4467 - accuracy: 0.8437 - val_loss: 0.4271 - val_accuracy: 0.8516\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 911us/step - loss: 0.4177 - accuracy: 0.8539 - val_loss: 0.4197 - val_accuracy: 0.8506\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 1s 823us/step - loss: 0.3984 - accuracy: 0.8599 - val_loss: 0.3921 - val_accuracy: 0.8656\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 1s 792us/step - loss: 0.3831 - accuracy: 0.8657 - val_loss: 0.3859 - val_accuracy: 0.8684\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3695 - accuracy: 0.8691 - val_loss: 0.3610 - val_accuracy: 0.8726\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3580 - accuracy: 0.8731 - val_loss: 0.3575 - val_accuracy: 0.8732\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 990us/step - loss: 0.3484 - accuracy: 0.8752 - val_loss: 0.3746 - val_accuracy: 0.8644\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 912us/step - loss: 0.3386 - accuracy: 0.8793 - val_loss: 0.3726 - val_accuracy: 0.8646\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 988us/step - loss: 0.3291 - accuracy: 0.8826 - val_loss: 0.3487 - val_accuracy: 0.8776\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 958us/step - loss: 0.3221 - accuracy: 0.8856 - val_loss: 0.3346 - val_accuracy: 0.8782\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 985us/step - loss: 0.3141 - accuracy: 0.8877 - val_loss: 0.3439 - val_accuracy: 0.8782\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3070 - accuracy: 0.8897 - val_loss: 0.3293 - val_accuracy: 0.8844\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2997 - accuracy: 0.8918 - val_loss: 0.3254 - val_accuracy: 0.8878\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 921us/step - loss: 0.2948 - accuracy: 0.8932 - val_loss: 0.3149 - val_accuracy: 0.8880\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 884us/step - loss: 0.2876 - accuracy: 0.8961 - val_loss: 0.3192 - val_accuracy: 0.8862\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 879us/step - loss: 0.2826 - accuracy: 0.8989 - val_loss: 0.3085 - val_accuracy: 0.8874\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2766 - accuracy: 0.8988 - val_loss: 0.3133 - val_accuracy: 0.8912\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2725 - accuracy: 0.9013 - val_loss: 0.3135 - val_accuracy: 0.8888\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2664 - accuracy: 0.9044 - val_loss: 0.3054 - val_accuracy: 0.8912\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2622 - accuracy: 0.9050 - val_loss: 0.3156 - val_accuracy: 0.8890\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2574 - accuracy: 0.9063 - val_loss: 0.3055 - val_accuracy: 0.8886\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 968us/step - loss: 0.2532 - accuracy: 0.9086 - val_loss: 0.3067 - val_accuracy: 0.8902\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 924us/step - loss: 0.2488 - accuracy: 0.9107 - val_loss: 0.2965 - val_accuracy: 0.8930\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 978us/step - loss: 0.2447 - accuracy: 0.9115 - val_loss: 0.3015 - val_accuracy: 0.8922\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2417 - accuracy: 0.9134 - val_loss: 0.2940 - val_accuracy: 0.8940\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2359 - accuracy: 0.9153 - val_loss: 0.3239 - val_accuracy: 0.8856\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2321 - accuracy: 0.9165 - val_loss: 0.3110 - val_accuracy: 0.8880\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2285 - accuracy: 0.9176 - val_loss: 0.3000 - val_accuracy: 0.8944\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=30, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1) # vertikale Bereich auf (0,1) setzten\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersagen Treffen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Model gibt uns die richtigen Werte zurück. Jetzt können wir das Model verwenden um Vorhersagen zu treffen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming model is your trained Keras model\n",
    "y_pred_probabilities = model.predict(X_new)\n",
    "y_pred_classes = np.argmax(y_pred_probabilities, axis=-1)\n",
    "np.array(class_names)[y_pred_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Das Gradientenverfahren***\n",
    "\n",
    "> - Der Gradientenabstieg (GD) ist ein iterativer Optimierungsalgorithmus erster Ordnung, der verwendet wird, um ein lokales Minimum/Maximum einer bestimmten Funktion zu finden. Diese Methode wird häufig beim maschinellen Lernen (ML) und Deep Learning (DL) verwendet, um eine Kosten-/Verlustfunktion zu minimieren (z. B. bei einer linearen Regression). Aufgrund seiner Bedeutung und einfachen Implementierung wird dieser Algorithmus in der Regel zu Beginn fast aller Kurse zum maschinellen Lernen gelehrt.\n",
    "\n",
    "- Allgemeiner Algoritmus zur Optimierung, der optimale Lösungen für eine Vielzahl von Fragestellungen ermitteln kann.\n",
    "Der Grundgedanke beim Gradientenverfahren ist, die Parameter iterativ so zu verändern das eine Kostenfunktion minimiert wird.\n",
    "\n",
    "# Gegen instable Gradienten kämpfen \n",
    "- Hyperparameter \n",
    "    - Droputout\n",
    "    - recurrent_dropout\n",
    "abschwächung der instablien Gradienten und das RNN effizienter Trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Allgemeiner Algoritmus zur Optimierung, der optimale Lösungen für eine Vielzahl von Fragestellungen ermitteln kann.\n",
    "Der Grundgedanke beim Gradientenverfahren ist, die Parameter iterativ so zu verändern das eine Kostenfunktion minimiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Batch Normalization nur bei der Eingabe also Input Layer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Das Problem des Kurzzeitgedächtnisses\n",
    "\n",
    "Aufgrund der Transformation, die die Daten auf ihrem Weg durch ein RNN durchlaufen, gehen bei jedem Zeitschritt manche Informationen verloren. Nach einiger Zeit enthält der Status RNN keinerlei Spuren der ursprünglichen Eingaben mehr. \n",
    ">BSP: Stellen wir uns vor jemand muss ein sehr langen Satz übersetzten - wenn Sie ihn fertig gelesen haben weiß sie nicht mehr, womit er begann.\n",
    "Um das Problem zu beheben wurde eine Langzeitgedächtnis vorgestellt mit sehr großem erfolg.\n",
    "\n",
    "Long Short Term Memory (LSTM)\n",
    "- Wie eine Blackbox vorstellen. \n",
    "- Training wird schneller Konvergieren und wird langfrisitge Abhängigkeiten erkennen in Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start RNN Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Rekurrente neuronale Netzwerke (RNNs) sind eine spezielle Art von neuronalen Netzwerken, die für die Verarbeitung sequenzieller Daten entwickelt wurden. Im Gegensatz zu herkömmlichen neuronalen Netzen haben RNNs eine \"Rekurrenz\" oder \"Rückkopplung\", die es ihnen ermöglicht, Informationen über die Zeit hinweg zu speichern und zu verarbeiten **return_Sequence=True, (LSTM)**. Diese Rückkopplungsschleife ermöglicht RNNs, Kontextinformationen zu berücksichtigen, was sie besonders gut für Aufgaben wie Sprachverarbeitung, Zeitreihenanalyse und Übersetzung macht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Arten von rekurrenten neuronalen Netzwerken (RNNs)***\n",
    "\n",
    "Eine Eingabesequenz ist einer Ausgabe zugeordnet. Sie können sie jedoch flexibel an verschiedene Konfigurationen für bestimmte Zwecke anpassen. Im Folgenden sind einige gänige RNN-Typen aufgeführt.\n",
    "#### Ein-zu-eins\n",
    "- Ein Beispiel für ein RNN mit einem one-to-one-Mapping ist ein einfaches Modell zur Klassifikation von Bildern. Die Eingabe des RNN ist ein Bild, das als 1D-Vektor dargestellt wird. Die rekurrente Schicht des RNN verarbeitet jeden Pixelwert des Bildes unabhängig voneinander, und die Ausgabeschicht gibt die Klassifikation des Bildes zurück, z.B. ob es einen Hund oder eine Katze zeigt.\n",
    "#### **Eins-zu-viele**\n",
    "- Dieser RNN-Typ kanalisiert einen Eingang an mehrere Ausgänge. Es ermöglicht sprachliche Anwendungen wie Bildunterschirften, indem ein Satz aus einem einzigen Schüsselwort generiert wird.\n",
    "\n",
    "#### **Viel-zu-viele**\n",
    "- Das Modell verwendet mehrere Eingaben, um mehrere Ausgaben vorherzusagen. Sie können beispielsweise einen Sprachübersetzter mit einem RNN erstellen, der einen Satz analysiert und die Wörter in einer anderen Sprache korrekt strukturiert.\n",
    "\n",
    "#### **Viel-zu-eins**\n",
    "- Mehrere Eingänge werden einem Ausgang zugeordet. Dies ist hilfreich bei Anwendungen wie der Stimmungsanalyse, bei der das Modell positive, negative und neutrale Stimmungen der Kunden anhand von Kundenreferenzen vorhersagt.\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/rnn_models.jpeg\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Blackbox***: \n",
    "\n",
    "- Als Blockbox wir in Data Science die Hiddenlayers gemeint die der Programmier nicht sehen. Da nicht nachvollziebar ist wie das Model den Output generiert. Das Auch Deepl Learning Neurale Netzwerke Hidden Layers nutzen und die für den Programmierer auch eine Blackbox ist (siehe bild)\n",
    "\n",
    "Und in der Backbox sind die einzelen Hidden Layers\n",
    "\n",
    "![Hidden Layer](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/hiddenlayer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Unterschied zwischen RNNs und FFN***\n",
    "1. Feedforward-Netzwerk:\n",
    "\n",
    "- In einem Feedforward-Netzwerk fließen die Daten nur in eine Richtung: von der Eingabeschicht über eine oder mehrere verdeckte Schichten bis zur Ausgabeschicht.\n",
    "Es gibt keine Rückkopplungsschleifen oder Verbindungen zwischen den Schichten, was bedeutet, dass die Neuronen in den Schichten keine Erinnerung an vergangene Zustände haben. Jedes Neuron in einer Schicht ist nur mit Neuronen in der vorherigen Schicht verbunden.\n",
    "Ein Feedforward-Netzwerk wird häufig für nicht-sequenzielle Daten verwendet, wie zum Beispiel für Bilderkennung oder Textklassifikation, wo jedes Datenstück unabhängig voneinander betrachtet werden kann.\n",
    "\n",
    "***2. Rekurrentes neuronales Netzwerk (RNN):***\n",
    "\n",
    "- Im Gegensatz dazu haben RNNs Rückkopplungsschleifen, die es ihnen ermöglichen, Informationen über die Zeit hinweg zu speichern und zu verwenden. Diese Rückkopplungsschleifen erlauben es, dass der Ausgang eines Neurons als Eingabe für sich selbst oder andere Neuronen in zukünftigen Zeitschritten dient.\n",
    "Diese Fähigkeit, zeitliche Abhängigkeiten zu modellieren, macht RNNs besonders gut geeignet für die Verarbeitung von sequenziellen Daten wie Texten, Zeitreihen oder Audiosignalen.\n",
    "Allerdings können herkömmliche RNNs Schwierigkeiten mit langen Zeitabständen haben, da das Problem des verschwindenden oder explodierenden Gradienten auftreten kann. Deshalb wurden verbesserte Varianten von RNNs wie Long Short-Term Memory (LSTM) und Gated Recurrent Unit (GRU) entwickelt, um einige dieser Probleme zu adressieren.\n",
    "Zusammengefasst, während Feedforward-Netzwerke Daten nur in eine Richtung propagieren und keine Rückkopplung haben, erlauben es RNNs, Informationen über die Zeit hinweg zu verarbeiten, was sie besonders geeignet für die Verarbeitung von Sequenzdaten macht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Backpropagation***\n",
    "\n",
    "\n",
    "Was macht Backpropagation ?\n",
    "- Bei jedem Trainingsdatenpunkt trifft der Backpropagation-Algorithmus zuerst eine Vohersage (im Vorwärtsdurchlauf), bestimmt den Fehler, durchläuft dann rückwärst jede Sicht, um den Fehlerbeitrag jeder Verbindung zuermitteln (im Rückwärtsdurchlauf), und verändert schließlich die Gewichte der Verbindungen, um den Fehler zu verringern (als Schritt im Gradientenverfahren)\n",
    "\n",
    "    - Es ist wichtig, alle Verbindungsgewichte der verborgenen Sichten mit Zufallswerte zu initialisieren, denn sonst wird das Training fehlschlagen. Initialisieren Sie zum Beispiel alle Gewichte und Bias mit null, werden alle Neuronen in einer Schicht identisch sein, und die Backpropagation wird sie exakt gleich behandeln, womit sie identisch bleiben.      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Weights and Bias***\n",
    "- In einem einfachen neuronalen Netzwerk wird die Berechnung ähnlich wie bei einer linearen Regression durchgeführt, bei der eine Ausgabe \n",
    "**y** anhand einer linearen Kombination der Eingaben **x**, eines Intercepts (Gewichts) und eines Slopes (Bias) berechnet wird. Der wesentliche Unterschied besteht darin, dass ein neuronales Netzwerk aus mehreren Schichten von Neuronen besteht, die miteinander verbunden sind und nichtlinear miteinander interagieren. Diese **nichtlineare Interaktion** wird durch die Verwendung einer **Aktivierungsfunktion** in jedem Neuron ermöglicht. Dadurch ist das neuronale Netzwerk in der Lage, **komplexere Beziehungen** zwischen den Eingaben und der Ausgabe zu erfassen und zu modellieren.\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/Terminologie_weights.png\" width=\"400\" />\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/Terminologie_baises.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function apply a nonlinear transformation and decide whether a neuron should be activated or not**\n",
    "\n",
    "\n",
    "- Ohne eine Aktivierungsfunktion ist es eine stacked Lineare Funktion zulösen. *Die Aktiverungsfunktion unbetten die funktion des Rnns*\n",
    "- wenn wir jedoch ein nicht Lineares Problem haben, brauchen wir eine nicht Lineare aktiverungs funktion\n",
    "\n",
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- Step Function\n",
    "- TanH\n",
    "- Leaky ReLU\n",
    "- Softmax\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "- Die Funktion **ReLU** ist eine Aktivierungs-funktion, die in neuronalen Netzten verwendet wird. Sie steht für **(Rectified Linear Unit)**. \n",
    "Diese Funktion gibt einfach den Eingabewert zurück, **wenn er positiv ist, und gibt 0 zurück, wenn er negativ ist**. Es ist eine weit verbreitete Aktivierungsfunktion aufgrund ihrer Einfachheit und ihrer guten Leistung in vielen Anwendungen.\n",
    "\n",
    "\n",
    "![Arten von Aktiverierungsfunktion](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/aktivierungsfunktionen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als **Faustregel** können Sie mit der Aktivierungsfunktion ReLU beginnen und dann zu anderen Aktivierungsfunktionen übergehen, wenn ReLU keine optimalen Ergebnisse liefert.\n",
    "\n",
    "Und hier sind noch ein paar andere Richtlinien, die Ihnen weiterhelfen.\n",
    "\n",
    "***Die ReLU-Aktivierungsfunktion sollte nur in den versteckten Schichten verwendet werden.***\n",
    "**Sigmoid/Logistic** und **Tanh-Funktionen** sollten **nicht in versteckten Schichten verwendet werden**, da sie das Modell während des Trainings anfälliger für Probleme machen (aufgrund verschwindender Gradienten).\n",
    "\n",
    "**Die Swish-Funktion wird in neuronalen Netzen mit einer Tiefe von mehr als 40 Schichten verwendet**.\n",
    "\n",
    "\n",
    "Abschließend noch ein paar Regeln für die Wahl der Aktivierungsfunktion für Ihre Ausgabeschicht, die auf der Art des zu lösenden Vorhersageproblems basiert:\n",
    "\n",
    "**Regression:** - Lineare Aktivierungsfunktion\n",
    "\n",
    "**Binäre Klassifizierung:** - Sigmoid/Logistische Aktivierungsfunktion\n",
    "\n",
    "**Multiklassen-Klassifikation:** - Softmax\n",
    "\n",
    "**Multilabel-Klassifikation:** - Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formel :\n",
    "\n",
    "\n",
    "![Beschreibung des Bildes](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/eq_29.png)\n",
    "\n",
    "\n",
    "\n",
    "- ***y(t)*** = Steht für die Ausgabe des neuronalen Netzes zur Zeit (t). \n",
    "Es repräsentiert das, was das Netzwerk zu einem bestimmten Zeitpunkt vorhersagt oder generiert.\n",
    "\n",
    "\n",
    "- **Wx** und **Wy** sind Gewichtsmatrizen, die die Verbindungen zwischen den Eigaben **Xt** und den vorherigen Ausgaben **yt−1** und den versteckten Schichten des neuronalen Netzes steuern. WXT und WTY bezeichne die Transponierten dieser Gewichtsmatrizen.\n",
    "\n",
    "- **Xt** ist die Eingabe des neuronales Netzes zum Zeitpunkt **t**. Sie repräsentiert die Daten oder Informationen, die dem Netzwerk zu diesem Zeitpunkt präsentiert werden.\n",
    "\n",
    "- **yt-1** ist die Ausgabe des neuronalen Netzes zum vorherigen Zeitpunkt t - 1. Sie wird verwendet, um die Informationen aus vergangenen Zustände zu berücksichtigen.\n",
    "\n",
    "- **b** ist der Bias-Vektor für die versteckten Schichten des neuronalen Netzes, ER ermöglicht es dem Netzwerk, Verschiebungen in den Daten zu berücksichtigen.\n",
    "\n",
    "Insgesamt wird die Ausgabe des neuronalen Netzes zum Zeitpunkt **t**\n",
    "als gewichtete Summe der Eingaben **Xt** und der vorherigen Ausgabe **yt-1** berechnet, die dann durch **die ReLU-Aktivierungsfunktion** gehen. Dieser Prozess wird verwendet, um Vorhersagen zu machen oder Daten zu generieren, basierend auf den Informationen zu diesem Zeitpunkt und vergangenen Zuständen des Netzwerks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Mean squared Error MSE***\n",
    "Der mittlere quadratische Fehler (MSE) ist eine gängige Metrik zur Bewertung der Leistung von Regressionsmodellen. Sie misst die durchschnittliche quadratische Abweichung zwischen den tatsächlichen und den vorhergesagten Werten.\n",
    "\n",
    "Hier ist eine schrittweise Erklärung des MSE:\n",
    "1. Differenz zwischen tatsächlichem und vorhergesagtem Wert: Für jede Instanz in den Daten wird die Differenz zwischen dem tatsächlichen Wert \n",
    "​und dem vorhergesagten Wert berechnet.\n",
    "\n",
    "2. Quadrat der Differenz: Die Differenzen werden quadriert. Dies stellt sicher, dass negative und positive Abweichungen gleich behandelt werden und dass größere Abweichungen stärker ins Gewicht fallen.\n",
    "\n",
    "3. Durchschnitt der quadrierten Differenzen: Die quadrierten Differenzen werden dann gemittelt, um den MSE zu erhalten. Dies geschieht, indem man die Summe der quadrierten Differenzen durch die Anzahl der Instanzen teilt.\n",
    "\n",
    "**Mathematisch ausgedrückt:**\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/mse.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "wo: \n",
    "n die anzahl \n",
    "yi der tatsächliche Wert den i-ten Instanz ist\n",
    "y^i der vorherergesagte Wert der i-ten Instanz ist.\n",
    "\n",
    "\n",
    "**WICHTIG:**\n",
    "- Ein niedriger MSE-Wert deutet drauf hin, dass das Modell gute Vorhersagen macht, da die tatsächlichen und vorhergesagten Werte eng beieinander liegen. \n",
    "\n",
    "- Ein hoher MSE-Wert deutet hingegen darauf hin, dass das Model schlechte Vorhersagen macht, da die Abweichung zwischen den tatäschlich und den vorhergesagten Werten groß ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung der funktion\n",
    "### MSE = 1/Anzahl der obversationen * Summe von (Die wahren Werte - die vorhergesagten Werte)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Flatten-Schicht: ist also wenn ich mir das Modul vorstellen das Input des Models\n",
    "Vorschläge anzeigen\n",
    "\n",
    "- Die Flatten-Schicht kann als \"Eingangstrichter\" des Modells betrachtet werden. Sie nimmt die Eingabedaten entgegen und formatiert sie so, dass sie von den folgenden Schichten des Modells verarbeitet werden können.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "- Die Flatten-Schicht nimmt Daten mit einer beliebigen Form entgegen und \"ebnet\" sie in einen eindimensionalen Vektor.\n",
    "Dies ist notwendig, da die meisten anderen Schichten des Modells mit Daten in Form von Vektoren arbeiten.\n",
    "Die Größe des Vektors hängt von der Anzahl der Merkmale in den Eingabedaten ab.\n",
    "Beispiel:\n",
    "\n",
    "- Angenommen, Sie haben Eingabedaten mit der Form (50, 10). Dies bedeutet, dass Sie 50 Datensätze mit jeweils 10 Merkmalen haben.\n",
    "\n",
    "- Die Flatten-Schicht würde diese Daten in einen Vektor mit 500 Elementen umwandeln.\n",
    "Jedes Element des Vektors würde einem Merkmal aus einem der 50 Datensätze entsprechen.\n",
    "Vorsichtsmaßnahmen:\n",
    "\n",
    "- Die Flatten-Schicht kann nur für Daten verwendet werden, die bereits in Form eines Arrays vorliegen.\n",
    "Wenn Ihre Daten in einem anderen Format vorliegen, müssen Sie sie vor der Verwendung der Flatten-Schicht in ein Array konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 15\n",
    "\n",
    "## 1. Verarbeiten von Sequenzen mit Rnns und Cnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rekurrente neuronale Netze (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendes wird im Kaptiel Thematisert\n",
    "1. Grundlegende Konzepte auf den RNN aufbaut und werden erfahren wie man sie per Backpropagation durch die Zeit trainiert\n",
    "2. Schwierigkeiten der Modelle Thematisieren\n",
    "    - Instablie Gradienten die durch verschiedene Techniken abgemildert werden können - unter anderem Recurrent Dropout und Recurrent Layer Normalization\n",
    "    - Ein sehr begrenztes Kurzeitgedächtnis das mithilfe von LSTM und GRU-Zellen erweiter werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2,offsets1, offsets2 = np.random.rand(4,batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))   # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # + Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellungs eines Traininigs einen Validierungs und einen Testdatensattz\n",
    "n_steps = 50\n",
    "\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000: -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wir haben also 10000 Reihen mit 50 Spalten\", series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Der X Test Datensatz besthet aus: \", X_train.shape, \"Der Y Test Datensatz besthet aus: \", y_train.shape)\n",
    "print(\"Der X Validierungs Datensatz besthet aus: \", X_valid.shape,\"Der Validierungs Datensatz besthet aus: \", y_valid.shape)\n",
    "print(\"Der X_test Datensatz besthet aus: \", X_test.shape,\"Der y_test Datensatz besteht  aus: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**Daten typ**\", type(X_train), type(y_train), type(X_valid), type(y_valid), type(X_test), type(y_test))\n",
    "# alle sind numpy arrays und haben die gleiche Anzahl an Dimensionen\n",
    "print(\"Shape der Daten\",X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Mean squared Error MSE***\n",
    "Der mittlere quadratische Fehler (MSE) ist eine gängige Metrik zur Bewertung der Leistung von Regressionsmodellen. Sie misst die durchschnittliche quadratische Abweichung zwischen den tatsächlichen und den vorhergesagten Werten.\n",
    "\n",
    "Hier ist eine schrittweise Erklärung des MSE:\n",
    "1. Differenz zwischen tatsächlichem und vorhergesagtem Wert: Für jede Instanz in den Daten wird die Differenz zwischen dem tatsächlichen Wert \n",
    "​und dem vorhergesagten Wert berechnet.\n",
    "\n",
    "2. Quadrat der Differenz: Die Differenzen werden quadriert. Dies stellt sicher, dass negative und positive Abweichungen gleich behandelt werden und dass größere Abweichungen stärker ins Gewicht fallen.\n",
    "\n",
    "3. Durchschnitt der quadrierten Differenzen: Die quadrierten Differenzen werden dann gemittelt, um den MSE zu erhalten. Dies geschieht, indem man die Summe der quadrierten Differenzen durch die Anzahl der Instanzen teilt.\n",
    "\n",
    "**Mathematisch ausgedrückt:**\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/mse.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "wo: \n",
    "n die anzahl \n",
    "yi der tatsächliche Wert den i-ten Instanz ist\n",
    "y^i der vorherergesagte Wert der i-ten Instanz ist.\n",
    "\n",
    "\n",
    "**WICHTIG:**\n",
    "- Ein niedriger MSE-Wert deutet drauf hin, dass das Modell gute Vorhersagen macht, da die tatsächlichen und vorhergesagten Werte eng beieinander liegen. \n",
    "\n",
    "- Ein hoher MSE-Wert deutet hingegen darauf hin, dass das Model schlechte Vorhersagen macht, da die Abweichung zwischen den tatäschlich und den vorhergesagten Werten groß ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung der funktion\n",
    "### MSE = 1/Anzahl der obversationen * Summe von (Die wahren Werte - die vorhergesagten Werte)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Flatten-Schicht: ist also wenn ich mir das Modul vorstellen das Input des Models\n",
    "Vorschläge anzeigen\n",
    "\n",
    "Die Flatten-Schicht kann als \"Eingangstrichter\" des Modells betrachtet werden. Sie nimmt die Eingabedaten entgegen und formatiert sie so, dass sie von den folgenden Schichten des Modells verarbeitet werden können.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "Die Flatten-Schicht nimmt Daten mit einer beliebigen Form entgegen und \"ebnet\" sie in einen eindimensionalen Vektor.\n",
    "Dies ist notwendig, da die meisten anderen Schichten des Modells mit Daten in Form von Vektoren arbeiten.\n",
    "Die Größe des Vektors hängt von der Anzahl der Merkmale in den Eingabedaten ab.\n",
    "Beispiel:\n",
    "\n",
    "Angenommen, Sie haben Eingabedaten mit der Form (50, 10). Dies bedeutet, dass Sie 50 Datensätze mit jeweils 10 Merkmalen haben.\n",
    "\n",
    "Die Flatten-Schicht würde diese Daten in einen Vektor mit 500 Elementen umwandeln.\n",
    "Jedes Element des Vektors würde einem Merkmal aus einem der 50 Datensätze entsprechen.\n",
    "Vorsichtsmaßnahmen:\n",
    "\n",
    "Die Flatten-Schicht kann nur für Daten verwendet werden, die bereits in Form eines Arrays vorliegen.\n",
    "Wenn Ihre Daten in einem anderen Format vorliegen, müssen Sie sie vor der Verwendung der Flatten-Schicht in ein Array konvertieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erklärung des Code abschnitt:**\n",
    "\n",
    "- **keras.Models.Sequential** leitet ein model ein\n",
    "- **keras.layer.Flatten** ist das Input, wenn man sich das Model vorsellt.\n",
    "- **keras.layer.Dense** ist das Output des Models. \n",
    "\n",
    "    - In unseren konrekten code ist noch keine Hidden layer eingebaut\n",
    "Somit können wir sehen das wir nur ein Input sowie Output haben des RNN Models. Jedoch noch keine Hiddenlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,1], name = \"Input\"),\n",
    "    keras.layers.Dense(1, name = \"Output\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In diesem Fall wird **mean_squared_error** verwendet, was den mittleren quadratischen Fehler berechnet. Diese Funktion eignet sich gut für Regressionsaufgaben, bei denen kontinuierliche Werte vorhergesagt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: ist ein Maß dafür wie gut oder schlecht Ihr Modell während des Trainings abschneidet. \n",
    "\n",
    "**model.compile(loss='mean_squared_error', optimizer='adam')** auch (MSE) genannt.\n",
    "In meinem Fall habe ich als *loss funktion* den **'mean_squared_error'** eingesetzt.\n",
    "\n",
    "- Höhe des Verlustes: Ein niedriger Verlustwert bedeutet, dass Ihr Modell gute Vorhersagen macht und die tatsächlichen Werte gut reproduziert. Ein hoher Verlustwert deutet darauf hin, dass Ihr Modell schlechte Vorhersagen macht und die tatsächlichen Werte nicht gut reproduziert.\n",
    "\n",
    "- Veränderung des Verlustes über die Epochen: Wenn der Verlust im Laufe des Trainings abnimmt, deutet dies darauf hin, dass Ihr Modell besser wird und die Vorhersagen genauer werden. Wenn der Verlust jedoch stagniert oder steigt, kann dies darauf hinweisen, dass Ihr Modell nicht mehr lernt oder überangepasst ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein einfaches RNN implementieren\n",
    "### SimpleRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape = [None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Rnns\n",
    "hier ist es ein Deep Rnn da wir ein hidden layers nutzen. Jedoch erstmal eine Linear Funktion da wir noch keine Aktivierungsfunktion haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(1, n_steps + 10)\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:] # X_new ist die Zeitreihe bis 50 und Y_new ist die Zeitreihe ab 50\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "Y_pred = X[:, n_steps:]\n",
    "# Berechnung des MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(keras.losses.mean_squared_error(Y_new, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seite 513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]  # 10 Schritte in die Zukunft\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.shape, X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(Y_valid, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seite 514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # jedes Ziel ist eine Sequenz von 10-D-Vektoren\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:,:, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]  \n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "model.compile(loss = 'mean_squared_error',optimizer='adam')\n",
    "model.fit(X_train, Y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die TimeDistributed-Schicht ermöglicht es, eine Schicht (wie z.B. eine Dichteschicht) auf jeden Zeitschritt dieser Sequenz anzuwenden. Das bedeutet, dass die Schicht auf jeden einzelnen Datenpunkt in der Sequenz angewendet wird, anstatt die gesamte Sequenz als Ganzes zu betrachten.\n",
    "\n",
    "In Ihrem Code wird die TimeDistributed-Schicht verwendet, um eine Dichteschicht (Dense-Schicht) auf jeden Zeitschritt der Ausgabe einer vorherigen SimpleRNN-Schicht anzuwenden. Dies erlaubt dem Modell, auf jede Zeitstufe der Ausgabe der vorherigen Schicht individuell zu reagieren und Muster in den Daten zu erkennen.\n",
    "\n",
    "Insgesamt ermöglicht die Verwendung der TimeDistributed-Schicht eine feinere Steuerung und Anpassung des Modells an die Daten, insbesondere in Sequenzdatensätzen wie Zeitreihen oder Textdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])\n",
    "model.fit(X_train, Y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Das Gradientenverfahren***\n",
    "\n",
    "- Allgemeiner Algoritmus zur Optimierung, der optimale Lösungen für eine Vielzahl von Fragestellungen ermitteln kann.\n",
    "Der Grundgedanke beim Gradientenverfahren ist, die Parameter iterativ so zu verändern das eine Kostenfunktion minimiert wird.\n",
    "\n",
    "# Gegen instable Gradienten kämpfen \n",
    "- Hyperparameter \n",
    "    - Droputout\n",
    "    - recurrent_dropout\n",
    "abschwächung der instablien Gradienten und das RNN effizienter Trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Batch Normalization nur bei der Eingabe also Input Layer***\n",
    "#### Das Problem des Kurzzeitgedächtnisses\n",
    "\n",
    "Aufgrund der Transformation, die die Daten auf ihrem Weg durch ein RNN durchlaufen, gehen bei jedem Zeitschritt manche Informationen verloren. Nach einiger Zeit enthält der Status RNN keinerlei Spuren der ursprünglichen Eingaben mehr. \n",
    ">BSP: Stellen wir uns vor jemand muss ein sehr langen Satz übersetzten - wenn Sie ihn fertig gelesen haben weiß sie nicht mehr, womit er begann.\n",
    "Um das Problem zu beheben wurde eine Langzeitgedächtnis vorgestellt mit sehr großem erfolg.\n",
    "\n",
    "### Long Short Term Memory (LSTM)\n",
    "- Wie eine Blackbox vorstellen. \n",
    "- Training wird schneller Konvergieren und wird langfrisitge Abhängigkeiten erkennen in Daten.\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/Rnn_LSTM_celle.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated-Rucurrent- Unit (GRU- ) GRU-Zellen\n",
    "- Ist eine vereinfachte Variante der LSTM - Zelle, die aber genauso gut zu funkoniert\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
