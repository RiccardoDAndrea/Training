{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlegende ideen bei einem Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Deep Neural Network (DNN) ist eine leistungsstarke Technik des maschinellen Lernens, die in verschiedenen Bereichen Anwendung findet. Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Muster in Daten zu erkennen und Vorhersagen oder Klassifikationen basierend auf diesen Mustern zu treffen. Hier sind einige der Schlüsselaspekte, die die Bedeutung von DNNs verdeutlichen:\n",
    "\n",
    "1. Mustererkennung: DNNs sind in der Lage, Muster in großen und komplexen Datensätzen zu erkennen, die von Menschen schwer oder unmöglich zu identifizieren sind. Dies macht sie besonders wertvoll in Anwendungen wie Bilderkennung, Spracherkennung und Natural Language Processing (NLP).\n",
    "\n",
    "2. Klassifikation und Vorhersage: DNNs ermöglichen die Klassifikation von Daten in verschiedene Kategorien oder die Vorhersage zukünftiger Ereignisse. Dies wird in Anwendungen wie Spam-Erkennung, Gesichtserkennung, medizinischer Diagnose und Finanzprognosen genutzt.\n",
    "\n",
    "3. Skalierbarkeit: DNNs können große Mengen an Daten verarbeiten und sind in der Lage, Modelle mit Tausenden oder Millionen von Parametern zu trainieren. Dies ermöglicht die Bewältigung komplexer Aufgaben und die Verarbeitung großer Datensätze.\n",
    "\n",
    "4. Automatisierung: DNNs können automatisch und kontinuierlich lernen, was bedeutet, dass sie sich an neue Daten und Veränderungen in der Umgebung anpassen können, ohne dass eine manuelle Anpassung erforderlich ist.\n",
    "\n",
    "5. Vielseitigkeit: DNNs finden in verschiedenen Bereichen Anwendung, darunter Bildverarbeitung, Sprachverarbeitung, autonome Fahrzeuge, medizinische Diagnostik, Empfehlungssysteme und vieles mehr. Sie sind eine Kernkomponente von Künstlicher Intelligenz (KI) und Deep Learning.\n",
    "\n",
    "6. Forschung und Innovation: DNNs haben zu bedeutenden Fortschritten in der KI-Forschung geführt und neue Möglichkeiten für innovative Anwendungen eröffnet. Sie sind ein Motor für die Entwicklung neuer Technologien und Lösungen.\n",
    "\n",
    "Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Aufgaben zu bewältigen und menschenähnliche Leistungen bei der Verarbeitung von Informationen zu erzielen. Dies hat Auswirkungen auf viele Branchen und trägt zur Transformation von Geschäftsmodellen und zur Lösung komplexer Herausforderungen bei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blackbox: Als Blockbox wir in Data Science die Hiddenlayers gemeint die der Programmier nicht sehen. Da nicht nachvollziebar ist wie das Model den Output generiert. Das Auch Deepl Learning Neurale Netzwerke Hidden Layers nutzen und die für den Programmierer auch eine Blackbox ist (siehe bild)\n",
    "\n",
    "![Hidden Layer](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/hiddenlayer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Arten von rekurrenten neuronalen Netzwerken (RNNs)**\n",
    "\n",
    "Eine Eingabesequenz ist einer Ausgabe zugeordnet. Sie können sie jedoch flexibel an verschiedene Konfigurationen für bestimmte Zwecke anpassen. Im Folgenden sind einige gänige RNN-Typen aufgeführt.\n",
    "\n",
    "#### Eins-zu-viele\n",
    "- Dieser RNN-Typ kanalisiert einen Eingang an mehrere Ausgänge. Es ermöglicht sprachliche Anwendungen wie Bildunterschirften, indem ein Satz aus einem einzigen Schüsselwort generiert wird.\n",
    "\n",
    "#### Viel-zu-viele\n",
    "- Das Modell verwendet mehrere Eingaben, um mehrere Ausgaben vorherzusagen. Sie können beispielsweise einen Sprachübersetzter mit einem RNN erstellen, der einen Satz analysiert und die Wörter in einer anderen Sprache korrekt strukturiert.\n",
    "\n",
    "#### Viel-zu-eins\n",
    "- Mehrere Eingänge werden einem Ausgang zugeordet. Dies ist hilfreich bei Anwendungen wie der Stimmungsanalyse, bei der das Modell positive, negative und neutrale Stimmungen der Kunden anhand von Kundenreferenzen vorhersagt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rekurrentes neuronales Netzwerk im Vergleich zu neuronalem Feed-Forward-Netzwerk\n",
    "\n",
    "- Wie Rnns sind neuronale Feed-Forward-Netze künstliche neuronale Netze, die Informationen von einem Ende zum Ende der Architektur weiterleiten. \n",
    "Ein neuuronales Feed-Forward-Netzwerk kann einfache Klassifizierungens-, Regressions- oder Erkennungsaufgaben ausführen, aber es kann sich nicht an die vorherige Eingabe erinnern, die es verarbeitet hat.\n",
    "\n",
    "- Zumbeispiel vergisst es \"Apfel\", wenn sein Neuron das Wort *ist* verarbeitet.\n",
    "Das RNN überwindet diese Speicherbeschränkung, indem es einen verborgenen Gedächtniszustand in das Neuron einbezieht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem **einfachen neuronalen Netzwerk** ist die Berechnung ähnlich wie bei einer **linearen Regression**, bei der eine Ausgabe \n",
    "**y** anhand einer linearen Kombination der Eingaben **x**, sowie eines **Intercepts (weight)** und eines **slope (Bias)** berechnet wird. Der Hauptunterschied besteht darin, dass ein neuronales Netzwerk mehrere Schichten von Neuronen hat, die nichtlinear miteinander verbunden sind, wodurch es komplexere Beziehungen zwischen Eingabe und Ausgabe modellieren kann. \n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/Terminologie_weights.png\" width=\"400\" />\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/Terminologie_baises.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "\n",
    "![Aktiverierungsfunktion](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/aktivierungsfunktionen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN) sind Neuornale Netzwerke. Die für die Verarbeitung squenzieller Daten entwickelt wuruden.\n",
    "\n",
    "Zwei wesentliche und wichtige Unterscheidungen \n",
    "- Feed-forward neural networks \n",
    "\n",
    "    - Neuronale Netze mit Feed-forward neural networks sind der häufigste Typ von neuronalen Netzen. Sie bestehen aus Schichten von Knoten, und die Informationen fließen in einer einzigen Richtung, von der Eingabeschicht (input) zur Ausgabeschicht (output).\n",
    "\n",
    "    - Jeder Knoten in einem neuronalen Feed-Forward-Netzwerk führt eine einfache Berechnung durch, und die Ausgabe jedes Knotens wird an den nächsten Knoten in der Schicht weitergeleitet.\n",
    "    \n",
    "    - Die Architektur eines einfachen RNN besteht aus einer Schleife, die es dem Netzwerk ermöglicht, Informationen aus vorherigen Schritten zu berücksichtigen. Dieser Rückkopplungsmechanismus erlaubt es dem Netzwerk, auf vergangene Zustände zu reagieren und somit eine gewisse Form von Gedächtnis zu haben. Ein Problem bei einfachen RNNs ist jedoch das Verschwinden oder Explodieren des Gradienten bei der Rückwärtspropagierung, was zu Schwierigkeiten beim Training führen kann.\n",
    "\n",
    "    - Was nutzen wir Feed-forward neural networks:\n",
    "    \n",
    "        -> Image Classification\n",
    "\n",
    "        -> Natural language processing \n",
    "        \n",
    "        -> speech recognition\n",
    "    - Nutzt man für Vorhersagen von continuous values (Bsp Aktienpreise vorhersagen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formel :\n",
    "\n",
    "\n",
    "![Beschreibung des Bildes](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/eq_29.png)\n",
    "\n",
    "\n",
    "\n",
    "- y(t) = Steht für die Ausgabe des neuronalen Netzes zur Zeit (t). \n",
    "Es repräsentiert das, was das Netzwerk zu einem bestimmten Zeitpunkt vorhersagt oder generiert.\n",
    "\n",
    "- Die Funktion **ReLU** ist eine Aktivierungs-funktion, die in neuronalen Netzten verwendet wird. Sie steht für **(Rectified Linear Unit)**. \n",
    "Diese Funktion gibt einfach den Eingabewert zurück, **wenn er positiv ist, und gibt 0 zurück, wenn er negativ ist**. Es ist eine weit verbreitete Aktivierungsfunktion aufgrund ihrer Einfachheit und ihrer guten Leistung in vielen Anwendungen.\n",
    "\n",
    "- **Wx** und **Wy** sind Gewichtsmatrizen, die die Verbindungen zwischen den Eigaben **Xt** und den vorherigen Ausgaben **yt−1** und den versteckten Schichten des neuronalen Netzes steuern. WXT und WTY bezeichne die Transponierten dieser Gewichtsmatrizen.\n",
    "\n",
    "- **Xt** ist die Eingabe des neuronales Netzes zum Zeitpunkt **t**. Sie repräsentiert die Daten oder Informationen, die dem Netzwerk zu diesem Zeitpunkt präsentiert werden.\n",
    "\n",
    "- **yt-1** ist die Ausgabe des neuronalen Netzes zum vorherigen Zeitpunkt t - 1. Sie wird verwendet, um die Informationen aus vergangenen Zustände zu berücksichtigen.\n",
    "\n",
    "- **b** ist der Bias-Vektor für die versteckten Schichten des neuronalen Netzes, ER ermöglicht es dem Netzwerk, Verschiebungen in den Daten zu berücksichtigen.\n",
    "\n",
    "Insgesamt wird die Ausgabe des neuronalen Netzes zum Zeitpunkt **t**\n",
    "als gewichtete Summe der Eingaben **Xt** und der vorherigen Ausgabe **yt-1** berechnet, die dann durch **die ReLU-Aktivierungsfunktion** gehen. Dieser Prozess wird verwendet, um Vorhersagen zu machen oder Daten zu generieren, basierend auf den Informationen zu diesem Zeitpunkt und vergangenen Zuständen des Netzwerks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "\n",
    "![Aktiverierungsfunktion](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/aktivierungsfunktionen.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 15\n",
    "\n",
    "## 1. Verarbeiten von Sequenzen mit Rnns und Cnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN = Rekurrente neuronale Netze\n",
    "CNN = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendes wird im Kaptiel Thematisert\n",
    "1. Grundlegende Konzepte auf den RNN aufbaut und werden erfahren wie man sie per Backpropagation durch die Zeit trainiert\n",
    "2. Schwierigkeiten der Modelle Thematisieren\n",
    "    - Instablie Gradienten die durch verschiedene Techniken abgemildert werden können - unter anderem Recurrent Dropout und Recurrent Layer Normalization\n",
    "    - Ein sehr begrenztes Kurzeitgedächtnis das mithilfe von LSTM und GRU-Zellen erweiter werden kann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Rnn sind nicht die einzigen Neuronal Netz die sequenzielle Daten verarbeiten können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2,offsets1, offsets2 = np.random.rand(4,batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))   # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # + Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellungs eines Traininigs einen Validierungs und einen Testdatensattz\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000: -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wir haben also 10000 Reihen mit 50 Spalten (10000, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Wir haben also 10000 Reihen mit 50 Spalten\", series.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der X Test Datensatz besthet aus:  (7000, 50, 1) Der Y Test Datensatz besthet aus:  (7000, 1)\n",
      "Der X Validierungs Datensatz besthet aus:  (2000, 50, 1) Der Validierungs Datensatz besthet aus:  (2000, 1)\n",
      "Der X_test Datensatz besthet aus:  (1000, 50, 1) Der y_test Datensatz besteht  aus:  (999, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Der X Test Datensatz besthet aus: \", X_train.shape, \"Der Y Test Datensatz besthet aus: \", y_train.shape)\n",
    "print(\"Der X Validierungs Datensatz besthet aus: \", X_valid.shape,\"Der Validierungs Datensatz besthet aus: \", y_valid.shape)\n",
    "print(\"Der X_test Datensatz besthet aus: \", X_test.shape,\"Der y_test Datensatz besteht  aus: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Daten typ** <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Shape der Daten (7000, 50, 1) (7000, 1) (2000, 50, 1) (2000, 1) (1000, 50, 1) (999, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"**Daten typ**\", type(X_train), type(y_train), type(X_valid), type(y_valid), type(X_test), type(y_test))\n",
    "# alle sind numpy arrays und haben die gleiche Anzahl an Dimensionen\n",
    "print(\"Shape der Daten\",X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020686228"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der mittlere quadratische Fehler (MSE) ist eine gängige Metrik zur Bewertung der Leistung von Regressionsmodellen. Sie misst die durchschnittliche quadratische Abweichung zwischen den tatsächlichen und den vorhergesagten Werten.\n",
    "\n",
    "Hier ist eine schrittweise Erklärung des MSE:\n",
    "1. Differenz zwischen tatsächlichem und vorhergesagtem Wert: Für jede Instanz in den Daten wird die Differenz zwischen dem tatsächlichen Wert \n",
    "​und dem vorhergesagten Wert berechnet.\n",
    "\n",
    "2. Quadrat der Differenz: Die Differenzen werden quadriert. Dies stellt sicher, dass negative und positive Abweichungen gleich behandelt werden und dass größere Abweichungen stärker ins Gewicht fallen.\n",
    "\n",
    "3. Durchschnitt der quadrierten Differenzen: Die quadrierten Differenzen werden dann gemittelt, um den MSE zu erhalten. Dies geschieht, indem man die Summe der quadrierten Differenzen durch die Anzahl der Instanzen teilt.\n",
    "\n",
    "Mathematisch ausgedrückt:\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/pictures_expl/mse.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "wo: \n",
    "n die anzahl \n",
    "yi der tatsächliche Wert den i-ten Instanz ist\n",
    "y^i der vorherergesagte Wert der i-ten Instanz ist.\n",
    "\n",
    "\n",
    "WICHTIG:\n",
    "- Ein niedriger MSE-Wert deutet drauf hin, dass das Modell gute Vorhersagen macht, da die tatsächlichen und vorhergesagten Werte eng beieinander liegen. \n",
    "\n",
    "- Ein hoher MSE-Wert deutet hingegen darauf hin, dass das Model schlechte Vorhersagen macht, da die Abweichung zwischen den tatäschlich und den vorhergesagten Werten groß ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Flatten-Schicht: ist also wenn ich mir das Modul vorstellen das Input des Models\n",
    "Vorschläge anzeigen\n",
    "\n",
    "Die Flatten-Schicht kann als \"Eingangstrichter\" des Modells betrachtet werden. Sie nimmt die Eingabedaten entgegen und formatiert sie so, dass sie von den folgenden Schichten des Modells verarbeitet werden können.\n",
    "\n",
    "Details:\n",
    "\n",
    "Die Flatten-Schicht nimmt Daten mit einer beliebigen Form entgegen und \"ebnet\" sie in einen eindimensionalen Vektor.\n",
    "Dies ist notwendig, da die meisten anderen Schichten des Modells mit Daten in Form von Vektoren arbeiten.\n",
    "Die Größe des Vektors hängt von der Anzahl der Merkmale in den Eingabedaten ab.\n",
    "Beispiel:\n",
    "\n",
    "Angenommen, Sie haben Eingabedaten mit der Form (50, 10). Dies bedeutet, dass Sie 50 Datensätze mit jeweils 10 Merkmalen haben.\n",
    "\n",
    "Die Flatten-Schicht würde diese Daten in einen Vektor mit 500 Elementen umwandeln.\n",
    "Jedes Element des Vektors würde einem Merkmal aus einem der 50 Datensätze entsprechen.\n",
    "Vorsichtsmaßnahmen:\n",
    "\n",
    "Die Flatten-Schicht kann nur für Daten verwendet werden, die bereits in Form eines Arrays vorliegen.\n",
    "Wenn Ihre Daten in einem anderen Format vorliegen, müssen Sie sie vor der Verwendung der Flatten-Schicht in ein Array konvertieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung des Code abschnitt:\n",
    "\n",
    "- **keras.Models.Sequential** leitet ein model ein\n",
    "- **keras.layer.Flatten** ist das Input, wenn man sich das Model vorsellt.\n",
    "- **keras.layer.Dense** ist das Output des Models. \n",
    "\n",
    "    -- In unseren konrekten code ist noch keine Hidden layer eingebaut\n",
    "Somit können wir sehen das wir nur ein Input sowie Output haben des RNN Models. Jedoch noch keine Hiddenlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,1], name = \"Input\"),\n",
    "    keras.layers.Dense(1, name = \"Output\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In diesem Fall wird **mean_squared_error** verwendet, was den mittleren quadratischen Fehler berechnet. Diese Funktion eignet sich gut für Regressionsaufgaben, bei denen kontinuierliche Werte vorhergesagt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 0s 305us/step - loss: 0.1465\n",
      "Epoch 2/20\n",
      "  1/219 [..............................] - ETA: 0s - loss: 0.0593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-02 15:07:19.079927: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 291us/step - loss: 0.0446\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 0s 294us/step - loss: 0.0275\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 0s 271us/step - loss: 0.0204\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 0s 275us/step - loss: 0.0166\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 0s 280us/step - loss: 0.0142\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 0s 280us/step - loss: 0.0123\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 0s 276us/step - loss: 0.0107\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 0s 278us/step - loss: 0.0094\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 0s 271us/step - loss: 0.0083\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 0s 280us/step - loss: 0.0073\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 0s 272us/step - loss: 0.0066\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 0s 270us/step - loss: 0.0060\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 0s 273us/step - loss: 0.0056\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 0s 270us/step - loss: 0.0052\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 276us/step - loss: 0.0049\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 0s 281us/step - loss: 0.0047\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 0s 282us/step - loss: 0.0045\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 0s 274us/step - loss: 0.0044\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 0s 278us/step - loss: 0.0043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10578ebe0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (Flatten)             (None, 50)                0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: ist ein Maß dafür wie gut oder schlecht Ihr Modell während des Trainings abschneidet. \n",
    "\n",
    "**model.compile(loss='mean_squared_error', optimizer='adam')** auch (MSE) genannt.\n",
    "In meinem Fall habe ich als *loss funktion* den **'mean_squared_error'** eingesetzt.\n",
    "\n",
    "- Höhe des Verlustes: Ein niedriger Verlustwert bedeutet, dass Ihr Modell gute Vorhersagen macht und die tatsächlichen Werte gut reproduziert. Ein hoher Verlustwert deutet darauf hin, dass Ihr Modell schlechte Vorhersagen macht und die tatsächlichen Werte nicht gut reproduziert.\n",
    "\n",
    "- Veränderung des Verlustes über die Epochen: Wenn der Verlust im Laufe des Trainings abnimmt, deutet dies darauf hin, dass Ihr Modell besser wird und die Vorhersagen genauer werden. Wenn der Verlust jedoch stagniert oder steigt, kann dies darauf hinweisen, dass Ihr Modell nicht mehr lernt oder überangepasst ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(keras.Model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ein einfaches RNN implementieren\n",
    "### SimpleRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SimpleRNN = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape = [None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SimpleRNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 2ms/step - loss: 0.2881\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1662\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0785\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0449\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0362\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0328\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0302\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0279\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0258\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0224\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0209\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0196\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0174\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.0165\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0157\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0149\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0143\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.0137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x152ca66d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SimpleRNN.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_SimpleRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 8ms/step - loss: 0.1538\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1448\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1443\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1435\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1432\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1433\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1431\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1431\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 0.1432\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1431\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1429\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.1428\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1429\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1426\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1426\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1422\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1422\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1422\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1421\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a333040>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deepRNN.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN_modifed = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN_modifed.compile(loss = 'mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 5ms/step - loss: 0.1501\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1458\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1446\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1440\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 7ms/step - loss: 0.1440\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1437\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1442\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1437\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1435\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1436\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1435\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1435\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1433\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1433\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1434\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1433\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1433\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16a333a30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deepRNN_modifed.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_5 (SimpleRNN)    (None, None, 20)          440       \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 20)                820       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_deepRNN_modifed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "series = generate_time_series(1, n_steps + 10)\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:] # X_new ist die Zeitreihe bis 50 und Y_new ist die Zeitreihe ab 50\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model_deepRNN_modifed.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "Y_pred = X[:, n_steps:]\n",
    "# Berechnung des MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.02598521, 0.00551254, 0.05561515, 0.11081391, 0.18151769,\n",
       "        0.29368085, 0.28848028, 0.2253627 , 0.16584933, 0.04296945]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = keras.losses.mean_squared_error(Y_new, Y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seite 513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]  # 10 Schritte in die Zukunft\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 2s 6ms/step - loss: 0.1610\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1441\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1439\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1437\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1434\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1434\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1432\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1430\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1430\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1431\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1430\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.1430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x176b5a250>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2843923"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0579928 , 0.05515712, 0.0330835 , 0.04777316, 0.04667747,\n",
       "        0.05513603, 0.07412457, 0.0660336 , 0.05350749, 0.05879381]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
