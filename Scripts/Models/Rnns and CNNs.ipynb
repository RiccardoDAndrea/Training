{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN) sind Neuornale Netzwerke. Die für die Verarbeitung squenzieller Daten entwickelt wuruden.\n",
    "\n",
    "Zwei wesentliche und wichtige Unterscheidungen \n",
    "- Feed-forward neural networks \n",
    "\n",
    "    - Neuronale Netze mit Feed-forward neural networks sind der häufigste Typ von neuronalen Netzen. Sie bestehen aus Schichten von Knoten, und die Informationen fließen in einer einzigen Richtung, von der Eingabeschicht (input) zur Ausgabeschicht (output).\n",
    "\n",
    "    - Jeder Knoten in einem neuronalen Feed-Forward-Netzwerk führt eine einfache Berechnung durch, und die Ausgabe jedes Knotens wird an den nächsten Knoten in der Schicht weitergeleitet.\n",
    "    \n",
    "    - Die Architektur eines einfachen RNN besteht aus einer Schleife, die es dem Netzwerk ermöglicht, Informationen aus vorherigen Schritten zu berücksichtigen. Dieser Rückkopplungsmechanismus erlaubt es dem Netzwerk, auf vergangene Zustände zu reagieren und somit eine gewisse Form von Gedächtnis zu haben. Ein Problem bei einfachen RNNs ist jedoch das Verschwinden oder Explodieren des Gradienten bei der Rückwärtspropagierung, was zu Schwierigkeiten beim Training führen kann.\n",
    "\n",
    "    - Was nutzen wir Feed-forward neural networks:\n",
    "    \n",
    "        -> Image Classification\n",
    "\n",
    "        -> Natural language processing \n",
    "        \n",
    "        -> speech recognition\n",
    "    - Nutzt man für Vorhersagen von continuous values (Bsp Aktienpreise vorhersagen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formel \n",
    "![Beschreibung des Bildes](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/eq_29.png)\n",
    "\n",
    "\n",
    "\n",
    "- y(t) = Steht für die Ausgabe des neuronalen Netzes zur Zeit (t). \n",
    "Es repräsentiert das, was das Netzwerk zu einem bestimmten Zeitpunkt vorhersagt oder generiert.\n",
    "\n",
    "- Die Funktion **ReLU** ist eine Aktivierungs-funktion, die in neuronalen Netzten verwendet wird. Sie steht für **(Rectified Linear Unit)**. \n",
    "Diese Funktion gibt einfach den Eingabewert zurück, **wenn er positiv ist, und gibt 0 zurück, wenn er negativ ist**. Es ist eine weit verbreitete Aktivierungsfunktion aufgrund ihrer Einfachheit und ihrer guten Leistung in vielen Anwendungen.\n",
    "\n",
    "- **Wx** und **Wy** sind Gewichtsmatrizen, die die Verbindungen zwischen den Eigaben **Xt** und den vorherigen Ausgaben **yt−1** und den versteckten Schichten des neuronalen Netzes steuern. WXT und WTY bezeichne die Transponierten dieser Gewichtsmatrizen.\n",
    "\n",
    "- **Xt** ist die Eingabe des neuronales Netzes zum Zeitpunkt **t**. Sie repräsentiert die Daten oder Informationen, die dem Netzwerk zu diesem Zeitpunkt präsentiert werden.\n",
    "\n",
    "- **yt-1** ist die Ausgabe des neuronalen Netzes zum vorherigen Zeitpunkt t - 1. Sie wird verwendet, um die Informationen aus vergangenen Zustände zu berücksichtigen.\n",
    "\n",
    "- **b** ist der Bias-Vektor für die versteckten Schichten des neuronalen Netzes, ER ermöglicht es dem Netzwerk, Verschiebungen in den Daten zu berücksichtigen.\n",
    "\n",
    "Insgesamt wird die Ausgabe des neuronalen Netzes zum Zeitpunkt **t**\n",
    "als gewichtete Summe der Eingaben **Xt** und der vorherigen Ausgabe **yt-1** berechnet, die dann durch **die ReLU-Aktivierungsfunktion** gehen. Dieser Prozess wird verwendet, um Vorhersagen zu machen oder Daten zu generieren, basierend auf den Informationen zu diesem Zeitpunkt und vergangenen Zuständen des Netzwerks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "\n",
    "![aktivierungsfunktionen](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/diags.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recurrent Neural Networks\n",
    "    - Recurrent neural networks are a type of neural network that has feedback loops. This means that the information can flow back and forth between the layers of the network.\n",
    "\n",
    "![Beschreibung des Bildes](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/aktivierungsfunktionen.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 15\n",
    "\n",
    "## 1. Verarbeiten von Sequenzen mit Rnns und Cnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN = Rekurrente neuronale Netze\n",
    "CNN = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendes wird im Kaptiel Thematisert\n",
    "1. Grundlegende Konzepte auf den RNN aufbaut und werden erfahren wie man sie per Backpropagation durch die Zeit trainiert\n",
    "2. Schwierigkeiten der Modelle Thematisieren\n",
    "    - Instablie Gradienten die durch verschiedene Techniken abgemildert werden können - unter anderem Recurrent Dropout und Recurrent Layer Normalization\n",
    "    - Ein sehr begrenztes Kurzeitgedächtnis das mithilfe von LSTM und GRU-Zellen erweiter werden kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Rnn sind nicht die einzigen Neuronal Netz die sequenzielle Daten verarbeiten können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
