{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grundlegende ideen bei einem Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Deep Neural Network (DNN) ist eine leistungsstarke Technik des maschinellen Lernens, die in verschiedenen Bereichen Anwendung findet. Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Muster in Daten zu erkennen und Vorhersagen oder Klassifikationen basierend auf diesen Mustern zu treffen. Hier sind einige der Schlüsselaspekte, die die Bedeutung von DNNs verdeutlichen:\n",
    "\n",
    "1. Mustererkennung: DNNs sind in der Lage, Muster in großen und komplexen Datensätzen zu erkennen, die von Menschen schwer oder unmöglich zu identifizieren sind. Dies macht sie besonders wertvoll in Anwendungen wie Bilderkennung, Spracherkennung und Natural Language Processing (NLP).\n",
    "\n",
    "2. Klassifikation und Vorhersage: DNNs ermöglichen die Klassifikation von Daten in verschiedene Kategorien oder die Vorhersage zukünftiger Ereignisse. Dies wird in Anwendungen wie Spam-Erkennung, Gesichtserkennung, medizinischer Diagnose und Finanzprognosen genutzt.\n",
    "\n",
    "3. Skalierbarkeit: DNNs können große Mengen an Daten verarbeiten und sind in der Lage, Modelle mit Tausenden oder Millionen von Parametern zu trainieren. Dies ermöglicht die Bewältigung komplexer Aufgaben und die Verarbeitung großer Datensätze.\n",
    "\n",
    "4. Automatisierung: DNNs können automatisch und kontinuierlich lernen, was bedeutet, dass sie sich an neue Daten und Veränderungen in der Umgebung anpassen können, ohne dass eine manuelle Anpassung erforderlich ist.\n",
    "\n",
    "5. Vielseitigkeit: DNNs finden in verschiedenen Bereichen Anwendung, darunter Bildverarbeitung, Sprachverarbeitung, autonome Fahrzeuge, medizinische Diagnostik, Empfehlungssysteme und vieles mehr. Sie sind eine Kernkomponente von Künstlicher Intelligenz (KI) und Deep Learning.\n",
    "\n",
    "6. Forschung und Innovation: DNNs haben zu bedeutenden Fortschritten in der KI-Forschung geführt und neue Möglichkeiten für innovative Anwendungen eröffnet. Sie sind ein Motor für die Entwicklung neuer Technologien und Lösungen.\n",
    "\n",
    "Die Bedeutung eines Deep Neural Networks liegt in seiner Fähigkeit, komplexe Aufgaben zu bewältigen und menschenähnliche Leistungen bei der Verarbeitung von Informationen zu erzielen. Dies hat Auswirkungen auf viele Branchen und trägt zur Transformation von Geschäftsmodellen und zur Lösung komplexer Herausforderungen bei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blackbox: Als Blockbox wir in Data Science das Model ansicher gemeint. Da nicht nachvollziebar ist wie das Model den Output generiert. Das Auch Deepl Learning Neurale Netzwerke Hidden Layers nutzen und die für den Programmierer auch eine Blackbox ist (siehe bild)\n",
    "![Hidden Layer](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/hiddenlayer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem **einfachen neuronalen Netzwerk** ist die Berechnung ähnlich wie bei einer **linearen Regression**, bei der eine Ausgabe \n",
    "**y** anhand einer linearen Kombination der Eingaben **x**, sowie eines **Intercepts (weight)** und eines **slope (Bias)** berechnet wird. Der Hauptunterschied besteht darin, dass ein neuronales Netzwerk mehrere Schichten von Neuronen hat, die nichtlinear miteinander verbunden sind, wodurch es komplexere Beziehungen zwischen Eingabe und Ausgabe modellieren kann. \n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/Terminologie_weights.png\" width=\"400\" />\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/Terminologie_baises.png\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "\n",
    "![Aktiverierungsfunktion](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/aktivierungsfunktionen.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNN) sind Neuornale Netzwerke. Die für die Verarbeitung squenzieller Daten entwickelt wuruden.\n",
    "\n",
    "Zwei wesentliche und wichtige Unterscheidungen \n",
    "- Feed-forward neural networks \n",
    "\n",
    "    - Neuronale Netze mit Feed-forward neural networks sind der häufigste Typ von neuronalen Netzen. Sie bestehen aus Schichten von Knoten, und die Informationen fließen in einer einzigen Richtung, von der Eingabeschicht (input) zur Ausgabeschicht (output).\n",
    "\n",
    "    - Jeder Knoten in einem neuronalen Feed-Forward-Netzwerk führt eine einfache Berechnung durch, und die Ausgabe jedes Knotens wird an den nächsten Knoten in der Schicht weitergeleitet.\n",
    "    \n",
    "    - Die Architektur eines einfachen RNN besteht aus einer Schleife, die es dem Netzwerk ermöglicht, Informationen aus vorherigen Schritten zu berücksichtigen. Dieser Rückkopplungsmechanismus erlaubt es dem Netzwerk, auf vergangene Zustände zu reagieren und somit eine gewisse Form von Gedächtnis zu haben. Ein Problem bei einfachen RNNs ist jedoch das Verschwinden oder Explodieren des Gradienten bei der Rückwärtspropagierung, was zu Schwierigkeiten beim Training führen kann.\n",
    "\n",
    "    - Was nutzen wir Feed-forward neural networks:\n",
    "    \n",
    "        -> Image Classification\n",
    "\n",
    "        -> Natural language processing \n",
    "        \n",
    "        -> speech recognition\n",
    "    - Nutzt man für Vorhersagen von continuous values (Bsp Aktienpreise vorhersagen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formel :\n",
    "\n",
    "\n",
    "![Beschreibung des Bildes](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/eq_29.png)\n",
    "\n",
    "\n",
    "\n",
    "- y(t) = Steht für die Ausgabe des neuronalen Netzes zur Zeit (t). \n",
    "Es repräsentiert das, was das Netzwerk zu einem bestimmten Zeitpunkt vorhersagt oder generiert.\n",
    "\n",
    "- Die Funktion **ReLU** ist eine Aktivierungs-funktion, die in neuronalen Netzten verwendet wird. Sie steht für **(Rectified Linear Unit)**. \n",
    "Diese Funktion gibt einfach den Eingabewert zurück, **wenn er positiv ist, und gibt 0 zurück, wenn er negativ ist**. Es ist eine weit verbreitete Aktivierungsfunktion aufgrund ihrer Einfachheit und ihrer guten Leistung in vielen Anwendungen.\n",
    "\n",
    "- **Wx** und **Wy** sind Gewichtsmatrizen, die die Verbindungen zwischen den Eigaben **Xt** und den vorherigen Ausgaben **yt−1** und den versteckten Schichten des neuronalen Netzes steuern. WXT und WTY bezeichne die Transponierten dieser Gewichtsmatrizen.\n",
    "\n",
    "- **Xt** ist die Eingabe des neuronales Netzes zum Zeitpunkt **t**. Sie repräsentiert die Daten oder Informationen, die dem Netzwerk zu diesem Zeitpunkt präsentiert werden.\n",
    "\n",
    "- **yt-1** ist die Ausgabe des neuronalen Netzes zum vorherigen Zeitpunkt t - 1. Sie wird verwendet, um die Informationen aus vergangenen Zustände zu berücksichtigen.\n",
    "\n",
    "- **b** ist der Bias-Vektor für die versteckten Schichten des neuronalen Netzes, ER ermöglicht es dem Netzwerk, Verschiebungen in den Daten zu berücksichtigen.\n",
    "\n",
    "Insgesamt wird die Ausgabe des neuronalen Netzes zum Zeitpunkt **t**\n",
    "als gewichtete Summe der Eingaben **Xt** und der vorherigen Ausgabe **yt-1** berechnet, die dann durch **die ReLU-Aktivierungsfunktion** gehen. Dieser Prozess wird verwendet, um Vorhersagen zu machen oder Daten zu generieren, basierend auf den Informationen zu diesem Zeitpunkt und vergangenen Zuständen des Netzwerks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt viele aktivierungsfunktionen für ein Neurales Network\n",
    "- ReLU (Rectified Linear Unit)\n",
    "- Softplus\n",
    "- Sigmoid \n",
    "\n",
    "![Aktiverierungsfunktion](/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/aktivierungsfunktionen.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 15\n",
    "\n",
    "## 1. Verarbeiten von Sequenzen mit Rnns und Cnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN = Rekurrente neuronale Netze\n",
    "CNN = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgendes wird im Kaptiel Thematisert\n",
    "1. Grundlegende Konzepte auf den RNN aufbaut und werden erfahren wie man sie per Backpropagation durch die Zeit trainiert\n",
    "2. Schwierigkeiten der Modelle Thematisieren\n",
    "    - Instablie Gradienten die durch verschiedene Techniken abgemildert werden können - unter anderem Recurrent Dropout und Recurrent Layer Normalization\n",
    "    - Ein sehr begrenztes Kurzeitgedächtnis das mithilfe von LSTM und GRU-Zellen erweiter werden kann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Rnn sind nicht die einzigen Neuronal Netz die sequenzielle Daten verarbeiten können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2,offsets1, offsets2 = np.random.rand(4,batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))   # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # + Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellungs eines Traininigs einen Validierungs und einen Testdatensattz\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000: -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der X Test Datensatz besthet aus:  (7000, 50, 1) Der Y Test Datensatz besthet aus:  (7000, 1)\n",
      "Der X Validierungs Datensatz besthet aus:  (2000, 50, 1) Der Validierungs Datensatz besthet aus:  (2000, 1)\n",
      "Der X_test Datensatz besthet aus:  (1000, 50, 1) Der y_test Datensatz besteht  aus:  (999, 51, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Der X Test Datensatz besthet aus: \", X_train.shape, \"Der Y Test Datensatz besthet aus: \", y_train.shape)\n",
    "print(\"Der X Validierungs Datensatz besthet aus: \", X_valid.shape,\"Der Validierungs Datensatz besthet aus: \", y_valid.shape)\n",
    "print(\"Der X_test Datensatz besthet aus: \", X_test.shape,\"Der y_test Datensatz besteht  aus: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021249553"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = X_valid[:,-1]\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der mittlere quadratische Fehler (MSE) ist eine gängige Metrik zur Bewertung der Leistung von Regressionsmodellen. Sie misst die durchschnittliche quadratische Abweichung zwischen den tatsächlichen und den vorhergesagten Werten.\n",
    "\n",
    "Hier ist eine schrittweise Erklärung des MSE:\n",
    "1. Differenz zwischen tatsächlichem und vorhergesagtem Wert: Für jede Instanz in den Daten wird die Differenz zwischen dem tatsächlichen Wert \n",
    "​und dem vorhergesagten Wert berechnet.\n",
    "\n",
    "2. Quadrat der Differenz: Die Differenzen werden quadriert. Dies stellt sicher, dass negative und positive Abweichungen gleich behandelt werden und dass größere Abweichungen stärker ins Gewicht fallen.\n",
    "\n",
    "3. Durchschnitt der quadrierten Differenzen: Die quadrierten Differenzen werden dann gemittelt, um den MSE zu erhalten. Dies geschieht, indem man die Summe der quadrierten Differenzen durch die Anzahl der Instanzen teilt.\n",
    "\n",
    "Mathematisch ausgedrückt:\n",
    "\n",
    "\n",
    "<img src=\"/Users/riccardo/Desktop/Repositorys_Github/Training/Scripts/Models/mse.png\" width=\"400\" />\n",
    "\n",
    "\n",
    "\n",
    "wo: \n",
    "n die anzahl \n",
    "yi der tatsächliche Wert den i-ten Instanz ist\n",
    "y^i der vorherergesagte Wert der i-ten Instanz ist.\n",
    "\n",
    "\n",
    "WICHTIG:\n",
    "- Ein niedriger MSE-Wert deutet drauf hin, dass das Modell gute Vorhersagen macht, da die tatsächlichen und vorhergesagten Werte eng beieinander liegen. \n",
    "\n",
    "- Ein hoher MSE-Wert deutet hingegen darauf hin, dass das Model schlechte Vorhersagen macht, da die Abweichung zwischen den tatäschlich und den vorhergesagten Werten groß ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/219 [..............................] - ETA: 35s - loss: 0.1241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 18:05:56.198714: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 0s 315us/step - loss: 0.0538\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 0s 275us/step - loss: 0.0157\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 0s 274us/step - loss: 0.0095\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 0s 272us/step - loss: 0.0075\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 0s 273us/step - loss: 0.0065\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 0s 270us/step - loss: 0.0057\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 0s 268us/step - loss: 0.0052\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 0s 266us/step - loss: 0.0048\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 0s 265us/step - loss: 0.0045\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 0s 270us/step - loss: 0.0043\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 0s 266us/step - loss: 0.0041\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 0s 267us/step - loss: 0.0040\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 0s 267us/step - loss: 0.0039\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 0s 266us/step - loss: 0.0038\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 0s 267us/step - loss: 0.0037\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 0s 269us/step - loss: 0.0036\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 0s 434us/step - loss: 0.0036\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 0s 269us/step - loss: 0.0035\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 0s 270us/step - loss: 0.0035\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 0s 277us/step - loss: 0.0034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b860850>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: ist ein Maß dafür wie gut oder schlecht Ihr Modell während des Trainings abschneidet. \n",
    "\n",
    "**model.compile(loss='mean_squared_error', optimizer='adam')** auch (MSE) genannt.\n",
    "In meinem Fall habe ich als *loss funktion* den **'mean_squared_error'** eingesetzt.\n",
    "\n",
    "- Höhe des Verlustes: Ein niedriger Verlustwert bedeutet, dass Ihr Modell gute Vorhersagen macht und die tatsächlichen Werte gut reproduziert. Ein hoher Verlustwert deutet darauf hin, dass Ihr Modell schlechte Vorhersagen macht und die tatsächlichen Werte nicht gut reproduziert.\n",
    "\n",
    "- Veränderung des Verlustes über die Epochen: Wenn der Verlust im Laufe des Trainings abnimmt, deutet dies darauf hin, dass Ihr Modell besser wird und die Vorhersagen genauer werden. Wenn der Verlust jedoch stagniert oder steigt, kann dies darauf hinweisen, dass Ihr Modell nicht mehr lernt oder überangepasst ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(keras.Model.compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SimpleRNN = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape = [None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SimpleRNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1420\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1420\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1414\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1415\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1416\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1410\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1412\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1411\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1416\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1415\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1411\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1413\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1413\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1413\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1420\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1411\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1415\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1410\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1413\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 2ms/step - loss: 0.1412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17bfbc580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_SimpleRNN.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_SimpleRNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Rnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 3s 8ms/step - loss: 0.0196\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0059\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0046\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0041\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0037\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0036\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0034\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0034\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0034\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0032\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0031\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0032\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0032\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0031\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0031\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0030\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0032\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0031\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0030\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17bc76d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deepRNN.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN_modifed = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepRNN_modifed.compile(loss = 'mean_squared_error',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "219/219 [==============================] - 2s 6ms/step - loss: 0.0163\n",
      "Epoch 2/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0043\n",
      "Epoch 3/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0036\n",
      "Epoch 4/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0033\n",
      "Epoch 5/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0032\n",
      "Epoch 6/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0031\n",
      "Epoch 7/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0031\n",
      "Epoch 8/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0029\n",
      "Epoch 9/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0030\n",
      "Epoch 10/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0030\n",
      "Epoch 11/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0030\n",
      "Epoch 12/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0028\n",
      "Epoch 13/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0029\n",
      "Epoch 14/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0028\n",
      "Epoch 15/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0028\n",
      "Epoch 16/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0028\n",
      "Epoch 17/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0027\n",
      "Epoch 18/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0027\n",
      "Epoch 19/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0027\n",
      "Epoch 20/20\n",
      "219/219 [==============================] - 1s 6ms/step - loss: 0.0027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17cffbbb0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deepRNN_modifed.fit(X_train, y_train,  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_6 (SimpleRNN)    (None, None, 20)          440       \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 20)                820       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_deepRNN_modifed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
