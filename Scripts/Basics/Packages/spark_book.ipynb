{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First real live project \n",
    "ETL Extract Transform Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark The Definitive Guide\n",
    "Starten einer 'sparkSession'.The SparkSession instance is the way Spark executes user-defined manipulations across the cluster.\n",
    "Code: \n",
    "- Input\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "Output:\n",
    "- SparkSession - in-memory\n",
    "\n",
    "    SparkContext\n",
    "\n",
    "    Spark UI\n",
    "\n",
    "    Version\n",
    "    v3.4.1\n",
    "    Master\n",
    "    local[*]\n",
    "    AppName\n",
    "    pyspark-shell\n",
    "\n",
    "\n",
    "\n",
    "###### Start Seite 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/25 21:21:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/25 21:21:02 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://air-von-ricca.fritz.box:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x147854700>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialisierung der Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann die Sparksession weiter detailiert betrachtet werden.\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen eine Spalte von 1000 Reihen Zahlen.\n",
    "Also 0 - 1.000\n",
    "Es wird ein Dataframe erstellt. \n",
    "Dieser Zahlenbereich stellt eine verteilte Sammlung dar. Wenn er in einem Cluster ausgeführt wird, existiert jeder Teil dieses Zahlenbereichs auf einem anderen Executor. Dies ist ein Spark DataFrame. \n",
    "- S.24 Mitte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    "- Ein DataFrame ist die gängigste strukturierte API und stellt einfach eine Datentabelle mit Zeilen und Spalten dar. Die Liste, die die Spalten und die Typen innerhalb dieser Spalten definiert, wird als Schema bezeichnet. \n",
    "Sie können sich einen DataFrame wie eine Tabellenkalkulation mit benannten Spalten vorstellen. \n",
    "\n",
    "- Eine Tabellenkalkulation befindet sich auf einem Computer an einem bestimmten Ort, während ein Spark DataFrame Tausende von Computern umfassen kann. Der Grund für die Speicherung der Daten auf mehr als einem Computer sollte intuitiv sein: Entweder sind die Daten zu groß, um auf einen Computer zu passen, oder es würde einfach zu lange dauern, diese Berechnung auf einem Computer durchzuführen.\n",
    "\n",
    "- Spark verfügt über mehrere zentrale Abstraktionen: Datensätze, DataFrames, SQL-Tabellen und belastbare verteilte Datensätze (RDDs). Diese verschiedenen Abstraktionen stellen alle verteilte Datensammlungen dar. Am einfachsten und effizientesten sind DataFrames, die in allen Sprachen verfügbar sind. Wir behandeln Datasets am Ende von Teil II, und RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partions \n",
    "\n",
    "- Damit jeder Executor die Arbeit parallel ausführen kann, unterteilt Spark die Daten in Stücke, die Partitionen genannt werden. \n",
    "Eine Partition ist eine Sammlung von Zeilen, die sich auf einer physischen Maschine in Ihrem Cluster befinden. Die Partitionen eines DataFrame stellen dar, wie die Daten während der Ausführung physisch über den Rechnercluster verteilt werden. Wenn Sie eine Partition haben, hat Spark nur eine Parallelität, auch wenn Sie Tausende von Executors haben. Wenn Sie viele Partitionen, aber nur einen Executor haben, hat Spark immer noch eine Parallelität von nur einer, da es nur eine Berechnungsressource gibt.\n",
    "\n",
    "--> (Partions oder auch Parallelität bedeutet wie viele Executor parallel auf den Dataset arbeiten können. Als Team wünsch man sich eine hohe Parallelität)>\n",
    "\n",
    "--> Im Kontext von Datenbanken können Partitionen verwendet werden, um eine große Tabelle in kleinere, handhabbare Segmente aufzuteilen. Diese Segmente können auf verschiedene Arten organisiert werden, z. B. nach bestimmten Werten in einer Spalte (z. B. nach Datum, Region oder Kundengruppe) oder nach bestimmten Kriterien (z. B. durch Zufallsverteilung).\n",
    "\n",
    "- Ein wichtiger Punkt ist, dass Sie mit DataFrames (in den meisten Fällen) Partitionen nicht manuell oder einzeln bearbeiten. Sie legen lediglich High-Level-Transformationen von Daten in den physischen Partitionen fest, und Spark bestimmt, wie diese Arbeit tatsächlich im Cluster ausgeführt wird. Es gibt APIs auf niedrigerer Ebene (über die RDD-Schnittstelle), die wir in Teil III behandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations \n",
    "\n",
    "In Spark sind die Kerndatenstrukturen unveränderlich, d. h. sie können nach ihrer Erstellung nicht mehr geändert werden. Dies mag auf den ersten Blick wie ein seltsames Konzept erscheinen: Wenn man sie nicht ändern kann, wie soll man sie dann verwenden? Um einen DataFrame zu \"ändern\", müssen Sie Spark mitteilen, wie Sie ihn ändern möchten, damit er das tut, was Sie wollen. Diese Anweisungen werden als Transformationen bezeichnet. Lassen Sie uns eine einfache Transformation durchführen, um alle geraden Zahlen in unserem aktuellen DataFrame zu finden:\n",
    "\n",
    "Zeile 36 führt aus das wir nur grade Zahlen bekommen jedoch wurde damit nicht der Kern datensatz verändert.\n",
    "Durch Zuweisung in eine andere Variable \"divisBy2\" haben wir den Kern immer noch nicht verändert, sondern die kurzzeitigen veränderten Datensatz in einem neue Variable\n",
    "\n",
    "=> Auch Transformation genannt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divisBy2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Beachten Sie, dass diese keine Ausgabe liefern. Dies liegt daran, dass wir nur eine abstrakte Transformation angegeben haben und Spark erst dann auf Transformationen reagiert, wenn wir eine Aktion aufrufen (dazu kommen wir gleich). Transformationen sind der Kern Ihrer Geschäftslogik in Spark. Es gibt zwei Arten von Transformationen: solche, die enge Abhängigkeiten angeben, und solche, die weite Abhängigkeiten angeben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Narrow Transformation \n",
    "- Wenn jede Partition im übergeordneten RDD von höchstens einer Partition des untergeordneten RDD verwendet wird, liegt eine enge Abhängigkeit vor. Berechnungen von Transformationen mit dieser Art von Abhängigkeit sind recht schnell, da sie keine Datenumlagerung über das Clusternetzwerk erfordern. Darüber hinaus sind auch Optimierungen wie Pipelining möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide Transformation\n",
    "Bei einer Transformation im Stil einer breiten Abhängigkeit (oder breiten Transformation) tragen Eingabepartitionen zu vielen Ausgabepartitionen bei. Dies wird oft als \"Shuffle\" bezeichnet, wobei Spark Partitionen im gesamten Cluster austauscht. Bei schmalen Transformationen führt Spark automatisch eine Operation durch, die als Pipelining bezeichnet wird, d. h., wenn wir mehrere Filter auf DataFrames angeben, werden sie alle im Speicher ausgeführt. Das Gleiche gilt nicht für Shuffles. Wenn wir einen Shuffle durchführen, schreibt Spark die Ergebnisse auf die Festplatte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lazy Evaluation\n",
    "\n",
    "Lazy Evaluation bedeutet, dass Spark bis zum letzten Moment wartet, um den Graphen der Berechnungsanweisungen auszuführen. Anstatt die Daten sofort zu ändern, wenn Sie eine Operation ausführen, erstellen Sie in Spark einen Plan mit Transformationen, die Sie auf Ihre Quelldaten anwenden möchten. Wenn Sie mit der Ausführung des Codes bis zur letzten Minute warten, kompiliert Spark diesen Plan aus Ihren rohen DataFrame-Transformationen zu einem rationalisierten physischen Plan, der so effizient wie möglich im Cluster ausgeführt wird. Dies bietet immense Vorteile, da Spark den gesamten\n",
    " \n",
    "gesamten Datenfluss von Ende zu Ende optimieren kann. Ein Beispiel hierfür ist das so genannte Prädikat Pushdown auf DataFrames. Wenn wir einen großen Spark-Job erstellen, aber am Ende einen Filter angeben, der nur eine Zeile aus unseren Quelldaten abrufen muss, ist der effizienteste Weg zur Ausführung der Zugriff auf den einzelnen benötigten Datensatz. Spark optimiert diesen Vorgang für uns, indem es den Filter automatisch nach unten verschiebt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Frist df:', 1000, 'The Second df:', 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The Frist df:\", myRange.count(), \"The Second df:\", divisBy2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.option(\"inferSchema\", \n",
    "                                   'true').option(\"header\", \n",
    "                                                  \"true\").csv(\"/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember, sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Let’s illustrate what’s happening when we call take on that resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#45 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#45 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=131]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#43,ORIGIN_COUNTRY_NAME#44,count#45] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie zuvor können wir nun eine Aktion festlegen, um diesen Plan zu starten. Bevor wir das tun, müssen wir jedoch noch eine Konfiguration festlegen. Wenn wir einen Shuffle durchführen, gibt Spark standardmäßig 200 Shuffle-Partitionen aus. Wir setzen diesen Wert auf 5, um die Anzahl der ausgegebenen Partitionen des Shuffle zu reduzieren:\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "  flightData2015.sort(\"count\").take(2)\n",
    "  ... Array([Vereinigte Staaten,Singapur,1], [Moldawien,Vereinigte Staaten,1])\n",
    "  \n",
    "Abbildung 2-9 veranschaulicht diesen Vorgang. Beachten Sie, dass zusätzlich zu den logischen Transformationen auch die Anzahl der physischen Partitionen berücksichtigt wird.\n",
    "\n",
    "csv File ->           Dataframe ->            -> Dataframe ->     Array()\n",
    "       Read (narrow)              sort (wide)             take(3)\n",
    "                      1 Partion                   5 Partion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createOrReplaceTempView(\"flight_data_2015\") <---- Var wo die Daten in der Sparsession gespeichert wurden\n",
    "\n",
    "Nun können wir unsere Daten in SQL abfragen. Dazu verwenden wir die Funktion spark.sql (zur Erinnerung: spark ist unsere SparkSession-Variable), die praktischerweise einen neuen DataFrame zurückgibt. Obwohl dies\n",
    "Logik ein wenig zirkulär erscheinen mag - dass eine SQL-Abfrage gegen einen DataFrame einen anderen DataFrame zurückgibt -, ist es tatsächlich ziemlich mächtig. Auf diese Weise können Sie Transformationen so spezifizieren, wie es für Sie zu einem bestimmten Zeitpunkt am bequemsten ist, ohne dass dies zu Lasten der Effizienz geht!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "  SELECT DEST_COUNTRY_NAME, count(1)\n",
    "  FROM flight_data_2015\n",
    "  GROUP BY DEST_COUNTRY_NAME\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#43, 5), ENSURE_REQUIREMENTS, [plan_id=153]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#43, 5), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#43] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Next Transformation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(COUNT)=370002)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT MAX(COUNT) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(COUNT)=370002)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.select(max(\"COUNT\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next Task\n",
    "\n",
    "\n",
    "--> find the top five destination countries in the data\n",
    "- Multitransformation query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, SUM(COUNT) AS destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY SUM (COUNT) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt für Schritt erklärung\n",
    "#### 1. Das Einlesen der Daten\n",
    "- Vorhin haben wir die Daten einer Variable zugeordnet \n",
    "-> Erinnernung Spark lies nicht die Daten bis eine \"action\" durch geführt wird oder om ursprünglichen DataFrame abgeleitete Aktion aufgerufen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Das Gruppieren der Daten.\n",
    "- Durch den befehl des GroupBy erstellen wir ein \"RealtionGroupedDataset\". Jedoch muss der nutzer noch eine Aggregation ansetzten bevor es weiter queried werden kann.\n",
    "Wie speziefizieren das wir ein KEY gruppieren oder ein \"set of key\" und das wir nun eine aggregation durch jeder dieser KEY/KEYS durchführen wollen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Aggregation spezifizieren\n",
    "- SUM() Aggregation. \n",
    "Dies nimmt eine Spalte oder simple ein Spalten name. Das Ergebnis durch das Summieren ist **ein neues Dataframe**. Es hat dardurch ein neues Schema jedoch weiß er auch den Typ jeder Spalte.\n",
    "\n",
    "- ***WICHTIG: Es ist wichtig, (noch einmal!) zu betonen, dass keine Berechnungen durchgeführt wurden***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Umbennen\n",
    "- Mit der \"withColumnRenamed\" methode welche druch zwei Argumente den alten Spalten und den neuen Spalten namen mit gegeben wird. \n",
    "- ***Auch wir keine Berechnung durch geführt. Es ist nur eine Transformation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sortierung der Daten\n",
    "- Hier legen wir fest das wir nur die höchsten Werte nehmen möchten des Dataframes das wir erstellt haben in schritt 1 bis 4 und in absteigender rheinfolge nach \"destination_total\" (eine spalte die wir neu erstellt haben mit der Gruppierung sowie Aggregation) es visulisiert haben möchten \n",
    "\n",
    "-- Column Types order column names sind die sonoynome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Speziefizierung wie viele Reihen das Dataframes sollen angezeigt werden.\n",
    ".limit(5)\n",
    "Zeigt uns nur die ersten 5 an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Ein Action durchführen\n",
    "\n",
    "- Hier beginnnen wir mit der Sammlung der Ergebnisse unserem Dataframes. Und Spark gibt uns eine Liste von Arrays in the Sprachen wo wir es gecoded haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#139L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#43,destination_total#139L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[sum(count#45)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#43, 5), ENSURE_REQUIREMENTS, [plan_id=336]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#43], functions=[partial_sum(count#45)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#43,count#45] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capter 3. A Tour of Sparks Toolset\n",
    "Inhalt von Kapitel 3\n",
    "\n",
    "- Running production applications with spark-submit Datasets: type-safe APIs for \n",
    "- structured data Structured Streaming\n",
    "- Machine learning and advanced analytics\n",
    "- Resilient Distributed Datasets (RDD): Spark’s low level APIs SparkR\n",
    "- The third-party package ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Production Applications \n",
    "\n",
    "\n",
    "Wichtiger Code: \n",
    "\n",
    "\n",
    "**\"Spark-submit\"**\n",
    "ermöglicht es Ihnen, Ihren Anwendungscode an einen Cluster zu senden und ihn dort auszuführen. Nach dem Senden wird die Anwendung ausgeführt, bis sie beendet wird (die Aufgabe abschließt) oder ein Fehler auftritt. Sie können dies mit allen von Spark unterstützten Clustermanagern tun, einschließlich Standalone, Mesos und YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Dataset \n",
    "https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/online-retail-dataset.csv\")\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "      .schema(staticSchema)\\\n",
    "      .option(\"maxFilesPerTrigger\", 1)\\\n",
    ".format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    ".load(\"/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/online-retail-dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingDataFrame.isStreaming #// returns true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\n",
    "      \"CustomerId\",\n",
    "      \"(UnitPrice * Quantity) as total_cost\",\n",
    "      \"InvoiceDate\")\\\n",
    "    .groupBy(\n",
    "      col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    ".sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/25 21:34:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/13/3pzxxwtd6cbdvdzq_lc6112r0000gn/T/temporary-56217fcc-af03-45b7-a226-629d191ba76d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/02/25 21:34:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x15127b760>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/25 21:34:59 ERROR MicroBatchExecution: Query customer_purchases [id = 90948c68-c4e4-4890-99de-69f80284e171, runId = 696167be-9fd2-4a1b-93c4-9a5615b0a027] terminated with error\n",
      "java.lang.IllegalArgumentException: Option 'basePath' must be a directory\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.basePaths(PartitioningAwareFileIndex.scala:266)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:197)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)\n",
      "\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSource.getBatch(FileStreamSource.scala:248)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$3(MicroBatchExecution.scala:586)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.foreach(StreamProgress.scala:27)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamProgress.flatMap(StreamProgress.scala:27)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$2(MicroBatchExecution.scala:582)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:582)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:284)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:411)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:409)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:247)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:306)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:284)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:207)\n"
     ]
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM customer_purchases\n",
    "ORDER BY `sum(total_cost)` DESC\n",
    "\"\"\")\\\n",
    ".show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
