{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 20:32:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "],  schema = 'a long, b double, c string, d date, e timestamp'\n",
    "\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  a|  b|  c|  d|  e|\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+\n",
      "|summary|                 a|                 b|      c|\n",
      "+-------+------------------+------------------+-------+\n",
      "|  count|                 3|                 3|      3|\n",
      "|   mean|2.3333333333333335|3.3333333333333335|   null|\n",
      "| stddev|1.5275252316519468|1.5275252316519468|   null|\n",
      "|    min|                 1|               2.0|string1|\n",
      "|    max|                 4|               5.0|string3|\n",
      "+-------+------------------+------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"a\",\"b\",\"c\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a ==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gro = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df_gro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First real live project \n",
    "ETL Extract Transform Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark The Definitive Guide\n",
    "Starten einer 'sparkSession'.The SparkSession instance is the way Spark executes user-defined manipulations across the cluster.\n",
    "Code: \n",
    "- Input\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "Output:\n",
    "- SparkSession - in-memory\n",
    "\n",
    "    SparkContext\n",
    "\n",
    "    Spark UI\n",
    "\n",
    "    Version\n",
    "    v3.4.1\n",
    "    Master\n",
    "    local[*]\n",
    "    AppName\n",
    "    pyspark-shell\n",
    "\n",
    "\n",
    "\n",
    "###### Start Seite 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://air-von-ricca.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x138ea46d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trainsched.txt\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier kann die Sparksession weiter detailiert betrachtet werden.\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen eine Spalte von 1000 Reihen Zahlen.\n",
    "Also 0 - 1.000\n",
    "Es wird ein Dataframe erstellt. \n",
    "Dieser Zahlenbereich stellt eine verteilte Sammlung dar. Wenn er in einem Cluster ausgeführt wird, existiert jeder Teil dieses Zahlenbereichs auf einem anderen Executor. Dies ist ein Spark DataFrame. \n",
    "- S.24 Mitte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames\n",
    "- Ein DataFrame ist die gängigste strukturierte API und stellt einfach eine Datentabelle mit Zeilen und Spalten dar. Die Liste, die die Spalten und die Typen innerhalb dieser Spalten definiert, wird als Schema bezeichnet. \n",
    "Sie können sich einen DataFrame wie eine Tabellenkalkulation mit benannten Spalten vorstellen. \n",
    "\n",
    "- Eine Tabellenkalkulation befindet sich auf einem Computer an einem bestimmten Ort, während ein Spark DataFrame Tausende von Computern umfassen kann. Der Grund für die Speicherung der Daten auf mehr als einem Computer sollte intuitiv sein: Entweder sind die Daten zu groß, um auf einen Computer zu passen, oder es würde einfach zu lange dauern, diese Berechnung auf einem Computer durchzuführen.\n",
    "\n",
    "- Spark verfügt über mehrere zentrale Abstraktionen: Datensätze, DataFrames, SQL-Tabellen und belastbare verteilte Datensätze (RDDs). Diese verschiedenen Abstraktionen stellen alle verteilte Datensammlungen dar. Am einfachsten und effizientesten sind DataFrames, die in allen Sprachen verfügbar sind. Wir behandeln Datasets am Ende von Teil II, und RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange = spark.range(1000).toDF(\"number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partions \n",
    "\n",
    "- Damit jeder Executor die Arbeit parallel ausführen kann, unterteilt Spark die Daten in Stücke, die Partitionen genannt werden. \n",
    "Eine Partition ist eine Sammlung von Zeilen, die sich auf einer physischen Maschine in Ihrem Cluster befinden. Die Partitionen eines DataFrame stellen dar, wie die Daten während der Ausführung physisch über den Rechnercluster verteilt werden. Wenn Sie eine Partition haben, hat Spark nur eine Parallelität, auch wenn Sie Tausende von Executors haben. Wenn Sie viele Partitionen, aber nur einen Executor haben, hat Spark immer noch eine Parallelität von nur einer, da es nur eine Berechnungsressource gibt.\n",
    "\n",
    "--> (Partions oder auch Parallelität bedeutet wie viele Executor parallel auf den Dataset arbeiten können. Als Team wünsch man sich eine hohe Parallelität)>\n",
    "\n",
    "--> Im Kontext von Datenbanken können Partitionen verwendet werden, um eine große Tabelle in kleinere, handhabbare Segmente aufzuteilen. Diese Segmente können auf verschiedene Arten organisiert werden, z. B. nach bestimmten Werten in einer Spalte (z. B. nach Datum, Region oder Kundengruppe) oder nach bestimmten Kriterien (z. B. durch Zufallsverteilung).\n",
    "\n",
    "- Ein wichtiger Punkt ist, dass Sie mit DataFrames (in den meisten Fällen) Partitionen nicht manuell oder einzeln bearbeiten. Sie legen lediglich High-Level-Transformationen von Daten in den physischen Partitionen fest, und Spark bestimmt, wie diese Arbeit tatsächlich im Cluster ausgeführt wird. Es gibt APIs auf niedrigerer Ebene (über die RDD-Schnittstelle), die wir in Teil III behandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations \n",
    "\n",
    "In Spark sind die Kerndatenstrukturen unveränderlich, d. h. sie können nach ihrer Erstellung nicht mehr geändert werden. Dies mag auf den ersten Blick wie ein seltsames Konzept erscheinen: Wenn man sie nicht ändern kann, wie soll man sie dann verwenden? Um einen DataFrame zu \"ändern\", müssen Sie Spark mitteilen, wie Sie ihn ändern möchten, damit er das tut, was Sie wollen. Diese Anweisungen werden als Transformationen bezeichnet. Lassen Sie uns eine einfache Transformation durchführen, um alle geraden Zahlen in unserem aktuellen DataFrame zu finden:\n",
    "\n",
    "Zeile 36 führt aus das wir nur grade Zahlen bekommen jedoch wurde damit nicht der Kern datensatz verändert.\n",
    "Durch Zuweisung in eine andere Variable \"divisBy2\" haben wir den Kern immer noch nicht verändert, sondern die kurzzeitigen veränderten Datensatz in einem neue Variable\n",
    "\n",
    "=> Auch Transformation genannt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number % 2 = 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "divisBy2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Beachten Sie, dass diese keine Ausgabe liefern. Dies liegt daran, dass wir nur eine abstrakte Transformation angegeben haben und Spark erst dann auf Transformationen reagiert, wenn wir eine Aktion aufrufen (dazu kommen wir gleich). Transformationen sind der Kern Ihrer Geschäftslogik in Spark. Es gibt zwei Arten von Transformationen: solche, die enge Abhängigkeiten angeben, und solche, die weite Abhängigkeiten angeben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Narrow Transformation \n",
    "- Wenn jede Partition im übergeordneten RDD von höchstens einer Partition des untergeordneten RDD verwendet wird, liegt eine enge Abhängigkeit vor. Berechnungen von Transformationen mit dieser Art von Abhängigkeit sind recht schnell, da sie keine Datenumlagerung über das Clusternetzwerk erfordern. Darüber hinaus sind auch Optimierungen wie Pipelining möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide Transformation\n",
    "Bei einer Transformation im Stil einer breiten Abhängigkeit (oder breiten Transformation) tragen Eingabepartitionen zu vielen Ausgabepartitionen bei. Dies wird oft als \"Shuffle\" bezeichnet, wobei Spark Partitionen im gesamten Cluster austauscht. Bei schmalen Transformationen führt Spark automatisch eine Operation durch, die als Pipelining bezeichnet wird, d. h., wenn wir mehrere Filter auf DataFrames angeben, werden sie alle im Speicher ausgeführt. Das Gleiche gilt nicht für Shuffles. Wenn wir einen Shuffle durchführen, schreibt Spark die Ergebnisse auf die Festplatte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lazy Evaluation\n",
    "\n",
    "Lazy Evaluation bedeutet, dass Spark bis zum letzten Moment wartet, um den Graphen der Berechnungsanweisungen auszuführen. Anstatt die Daten sofort zu ändern, wenn Sie eine Operation ausführen, erstellen Sie in Spark einen Plan mit Transformationen, die Sie auf Ihre Quelldaten anwenden möchten. Wenn Sie mit der Ausführung des Codes bis zur letzten Minute warten, kompiliert Spark diesen Plan aus Ihren rohen DataFrame-Transformationen zu einem rationalisierten physischen Plan, der so effizient wie möglich im Cluster ausgeführt wird. Dies bietet immense Vorteile, da Spark den gesamten\n",
    " \n",
    "gesamten Datenfluss von Ende zu Ende optimieren kann. Ein Beispiel hierfür ist das so genannte Prädikat Pushdown auf DataFrames. Wenn wir einen großen Spark-Job erstellen, aber am Ende einen Filter angeben, der nur eine Zeile aus unseren Quelldaten abrufen muss, ist der effizienteste Weg zur Ausführung der Zugriff auf den einzelnen benötigten Datensatz. Spark optimiert diesen Vorgang für uns, indem es den Filter automatisch nach unten verschiebt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Frist df:', 1000, 'The Second df:', 500)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The Frist df:\", myRange.count(), \"The Second df:\", divisBy2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.option(\"inferSchema\", \n",
    "                                   'true').option(\"header\", \n",
    "                                                  \"true\").csv(\"/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember, sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Let’s illustrate what’s happening when we call take on that resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#3767 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#3767 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=452]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#3765,ORIGIN_COUNTRY_NAME#3766,count#3767] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/2015-..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie zuvor können wir nun eine Aktion festlegen, um diesen Plan zu starten. Bevor wir das tun, müssen wir jedoch noch eine Konfiguration festlegen. Wenn wir einen Shuffle durchführen, gibt Spark standardmäßig 200 Shuffle-Partitionen aus. Wir setzen diesen Wert auf 5, um die Anzahl der ausgegebenen Partitionen des Shuffle zu reduzieren:\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "  flightData2015.sort(\"count\").take(2)\n",
    "  ... Array([Vereinigte Staaten,Singapur,1], [Moldawien,Vereinigte Staaten,1])\n",
    "  \n",
    "Abbildung 2-9 veranschaulicht diesen Vorgang. Beachten Sie, dass zusätzlich zu den logischen Transformationen auch die Anzahl der physischen Partitionen berücksichtigt wird.\n",
    "\n",
    "csv File ->           Dataframe ->            -> Dataframe ->     Array()\n",
    "       Read (narrow)              sort (wide)             take(3)\n",
    "                      1 Partion                   5 Partion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createGlobalTempView(\"flightData2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+------+------+----+----+----+----+-------+----+----+----+----+----+----+----+----+---+\n",
      "|STATIONS_ID|MESS_DATUM| QN_3|    FX|    FM|QN_4| RSK|RSKF| SDK|SHK_TAG|  NM| VPM|  PM| TMK| UPM| TXK| TNK| TGK|eor|\n",
      "+-----------+----------+-----+------+------+----+----+----+----+-------+----+----+----+----+----+----+----+----+---+\n",
      "|       1766|  19820101|   10|   6.7|   2.4|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820102|   10|  10.3|   4.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820103|   10|  13.9|   5.6|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820104|   10|  18.0|   7.1|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820105|   10|  14.4|   6.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820106|   10|  17.5|   7.3|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820107|   10|   4.1|   2.2|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820108|   10|  12.9|   6.3|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820109|   10|  13.4|   5.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820110|   10|   8.2|   3.8|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820111|   10|   8.2|   2.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820112|   10|   3.1|   1.2|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820113|   10|   3.6|   1.3|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820114|   10|   4.6|   2.3|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820115|   10|   5.7|   2.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820116|   10|   7.2|   3.1|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820117|   10|   3.1|   1.6|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820118|   10|   3.6|   1.9|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820119|   10|   4.6|   2.5|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "|       1766|  19820120|   10|   4.1|   2.4|-999|-999|-999|-999|   -999|-999|-999|-999|-999|-999|-999|-999|-999|eor|\n",
      "+-----------+----------+-----+------+------+----+----+----+----+-------+----+----+----+----+----+----+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = \"/Users/riccardo/Desktop/Repositorys_Github/Training/Dataset/produkt_klima_tag_19820101_20221231_01766.txt\"\n",
    "\n",
    "spark_df = spark.read.option(\"sep\", \";\").option(\"header\", \"true\").csv(txt_file_path)\n",
    "\n",
    "# Create temporary table called table1\n",
    "spark_df.createOrReplaceTempView(\"table1\")\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+----+----+\n",
      "|STATIONS_ID|MESS_DATUM|QN_3|  FX|  FM|QN_4| RSK|RSKF| SDK|SHK_TAG|  NM| VPM|  PM| TMK| UPM| TXK| TNK| TGK| eor|\n",
      "+-----------+----------+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+----+----+\n",
      "|       1766|  19891001|  10|   9|   3|  10|   0|   6|   0|      0|   7|  13|1020|  12|  92|  15|  10|  10|null|\n",
      "|       1766|  19891002|  10|  12|   4|  10|   0|   6|   2|      0|   7|  11|1016|  12|  77|  15|  10|  10|null|\n",
      "|       1766|  19891003|  10|   9|   2|  10|   0|   0|  10|      0|   1|   9|1018|   8|  77|  15|   4|   8|null|\n",
      "|       1766|  19891004|  10|   3|   0|  10|   0|   0|  10|      0|   0|   7|1021|   6|  77|  15|  -1|  -2|null|\n",
      "|       1766|  19891005|  10|   7|   2|  10|   0|   6|  10|      0|   1|   9|1016|  11|  70|  19|   3|   1|null|\n",
      "|       1766|  19891006|  10|  11|   4|  10|  11|   6|   3|      0|   7|  11|1009|  11|  83|  15|   8|   5|null|\n",
      "|       1766|  19891007|  10|   9|   4|  10|  11|   6|   0|      0|   7|  10|1001|   9|  90|  12|   8|   8|null|\n",
      "|       1766|  19891008|  10|   7|   2|  10|   0|   6|   0|      0|   7|  10|1007|   9|  93|  11|   6|   5|null|\n",
      "|       1766|  19891009|  10|   6|   2|  10|   2|   6|   0|      0|   6|  11|1008|   9|  95|  11|   8|   7|null|\n",
      "|       1766|  19891010|  10|   9|   3|  10|   1|   6|   3|      0|   6|   9|1010|   8|  83|  12|   5|   5|null|\n",
      "+-----------+----------+----+----+----+----+----+----+----+-------+----+----+----+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark_df.filter((spark_df.RSKF != -999) &\n",
    "                       (spark_df.MESS_DATUM >= 19891001) & \n",
    "                       (spark_df.MESS_DATUM <= 19891010))\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 12:40:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-----+\n",
      "|summary|STATIONS_ID|          MESS_DATUM|              QN_3|                FX|                FM|              QN_4|               RSK|               RSKF|                SDK|            SHK_TAG|                 NM|                VPM|               PM|                TMK|               UPM|                TXK|               TNK|               TGK|  eor|\n",
      "+-------+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-----+\n",
      "|  count|      14944|               14944|             14944|             14944|             14944|             14944|             14944|              14944|              14944|              14944|              14944|              14944|            14944|              14944|             14944|              14944|             14944|             14944|14944|\n",
      "|   mean|     1766.0|2.0021082812633835E7|  8.77154710920771|4.9852047644539335|0.6326485546038545|-179.2706771948608|-185.4955835117734|-183.68274892933619|-183.59553024624998|-186.98608137044968|-182.95018067452503|-178.87117237687195|633.1262145342607|-178.70155915417578|-123.8276472162748|-175.27344753747187|-182.1407253747325|-183.7799652034265| null|\n",
      "| stddev|        0.0|  118095.09008812868|27.410433510067982| 72.84842226545345|52.461335546111314| 393.5413356292764|390.56973854156536|  391.4299461264182| 391.56745691580267| 389.83656629969965|  392.0349700635927| 393.83246144685745|783.6034914013961| 393.86030889986677| 420.3899411860675| 395.52417583278856| 392.1999551665802|391.41596628526037| null|\n",
      "|    min|       1766|            19820101|                 3|               1.4|               0.0|                 3|               0.0|                  0|              0.000|                  0|                0.0|                1.5|           968.60|                0.0|             32.00|                0.0|               0.0|               0.0|  eor|\n",
      "|    max|       1766|            20221231|              -999|              -999|              -999|              -999|              -999|               -999|               -999|               -999|               -999|               -999|             -999|               -999|              -999|               -999|              -999|              -999|  eor|\n",
      "+-------+-----------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+-------------------+------------------+-------------------+------------------+------------------+-----+\n",
      "\n",
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |-- QN_3: string (nullable = true)\n",
      " |--   FX: string (nullable = true)\n",
      " |--   FM: string (nullable = true)\n",
      " |-- QN_4: string (nullable = true)\n",
      " |--  RSK: string (nullable = true)\n",
      " |-- RSKF: string (nullable = true)\n",
      " |--  SDK: string (nullable = true)\n",
      " |-- SHK_TAG: string (nullable = true)\n",
      " |--   NM: string (nullable = true)\n",
      " |--  VPM: string (nullable = true)\n",
      " |--   PM: string (nullable = true)\n",
      " |--  TMK: string (nullable = true)\n",
      " |--  UPM: string (nullable = true)\n",
      " |--  TXK: string (nullable = true)\n",
      " |--  TNK: string (nullable = true)\n",
      " |--  TGK: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['STATIONS_ID',\n",
       " 'MESS_DATUM',\n",
       " 'QN_3',\n",
       " '  FX',\n",
       " '  FM',\n",
       " 'QN_4',\n",
       " ' RSK',\n",
       " 'RSKF',\n",
       " ' SDK',\n",
       " 'SHK_TAG',\n",
       " '  NM',\n",
       " ' VPM',\n",
       " '  PM',\n",
       " ' TMK',\n",
       " ' UPM',\n",
       " ' TXK',\n",
       " ' TNK',\n",
       " ' TGK',\n",
       " 'eor']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.describe().show()\n",
    "spark_df.printSchema()\n",
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: integer (nullable = true)\n",
      " |-- MESS_DATUM: integer (nullable = true)\n",
      " |-- QN_3: integer (nullable = true)\n",
      " |--   FX: integer (nullable = true)\n",
      " |--   FM: integer (nullable = true)\n",
      " |-- QN_4: integer (nullable = true)\n",
      " |--  RSK: integer (nullable = true)\n",
      " |-- RSKF: integer (nullable = true)\n",
      " |--  SDK: integer (nullable = true)\n",
      " |-- SHK_TAG: integer (nullable = true)\n",
      " |--   NM: integer (nullable = true)\n",
      " |--  VPM: integer (nullable = true)\n",
      " |--   PM: integer (nullable = true)\n",
      " |--  TMK: integer (nullable = true)\n",
      " |--  UPM: integer (nullable = true)\n",
      " |--  TXK: integer (nullable = true)\n",
      " |--  TNK: integer (nullable = true)\n",
      " |--  TGK: integer (nullable = true)\n",
      " |-- eor: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spalten = spark_df.columns\n",
    "for spalte in spalten:\n",
    "    spark_df.withColumn(spalte, col(spalte).cast(\"integer\"))\n",
    "\n",
    "# DataFrame anzeigen\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATIONS_ID: string (nullable = true)\n",
      " |-- MESS_DATUM: string (nullable = true)\n",
      " |-- QN_3: string (nullable = true)\n",
      " |--   FX: string (nullable = true)\n",
      " |--   FM: string (nullable = true)\n",
      " |-- QN_4: string (nullable = true)\n",
      " |--  RSK: string (nullable = true)\n",
      " |-- RSKF: string (nullable = true)\n",
      " |--  SDK: string (nullable = true)\n",
      " |-- SHK_TAG: string (nullable = true)\n",
      " |--   NM: string (nullable = true)\n",
      " |--  VPM: string (nullable = true)\n",
      " |--   PM: string (nullable = true)\n",
      " |--  TMK: string (nullable = true)\n",
      " |--  UPM: string (nullable = true)\n",
      " |--  TXK: string (nullable = true)\n",
      " |--  TNK: string (nullable = true)\n",
      " |--  TGK: string (nullable = true)\n",
      " |-- eor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(\"SELECT * FROM table1\")\n",
    "result_df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
