{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://air-von-ricca.fritz.box:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109d69f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "],  schema = 'a long, b double, c string, d date, e timestamp'\n",
    "\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "|  a|  b|  c|  d|  e|\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "only showing top 0 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+-------+----------+-------------------+\n",
       "|  a|  b|      c|         d|                  e|\n",
       "+---+---+-------+----------+-------------------+\n",
       "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
       "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
       "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
       "+---+---+-------+----------+-------------------+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+\n",
      "|summary|                 a|                 b|      c|\n",
      "+-------+------------------+------------------+-------+\n",
      "|  count|                 3|                 3|      3|\n",
      "|   mean|2.3333333333333335|3.3333333333333335|   null|\n",
      "| stddev|1.5275252316519468|1.5275252316519468|   null|\n",
      "|    min|                 1|               2.0|string1|\n",
      "|    max|                 4|               5.0|string3|\n",
      "+-------+------------------+------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"a\",\"b\",\"c\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a ==1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gro = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df_gro.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First real live project \n",
    "ETL Extract Transform Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+----------+----------+--------------------+-------------+-------+--------+---+\n",
      "|            order_id|      date|               email|first_name| last_name|             address|      country|   item|    size|qty|\n",
      "+--------------------+----------+--------------------+----------+----------+--------------------+-------------+-------+--------+---+\n",
      "|01HPYDWKXGEBJD9ZJ...| 7/19/2023|  brieger0@skype.com|       Bat|    Rieger|8529 Park Meadow ...|United States|  Hoody|XX-Large|  2|\n",
      "|01HPYDWKZDZCTC2TP...| 7/10/2023|   mabbitt1@bing.com|      Mary|    Abbitt|     8 Debs Junction|       Norway| Gloves|  Medium|  1|\n",
      "|01HPYDWKZGM48TRP1...| 10/7/2023|ckeave2@bravesite...|   Cordula|     Keave|  618 Packers Avenue|       Brazil|Sweater| X-Large|  2|\n",
      "|01HPYDWKZJ71D89EN...| 9/27/2023| rgymlett3@bbc.co.uk|     Rhoda|   Gymlett|       30 Stang Lane|       Sweden|  Hoody|XX-Large|  1|\n",
      "|01HPYDWKZNDT89QSV...|  2/8/2023|   mdorre4@google.cn|      Mead|     Dorre|09 Rockefeller Ci...|    Argentina| Tshirt|   Large|  4|\n",
      "|01HPYDWKZRGTNP8ZH...| 4/12/2023|mmabbot5@state.tx.us|    Menard|    Mabbot|    02377 Truax Lane|       Brazil| Tshirt|XX-Large|  4|\n",
      "|01HPYDWKZTPXTDX72...| 3/16/2023| asprott6@netlog.com|   Abelard|    Sprott|16446 Loftsgordon...|       Brazil| Tshirt|XX-Large|  4|\n",
      "|01HPYDWKZX2YEF9DS...|10/14/2023|cbestman7@tinyurl...|       Cly|   Bestman|    40 Goodland Park|         Peru| Tshirt|   Small|  4|\n",
      "|01HPYDWM00BJQ09K2...| 6/30/2023|   kfilshin8@ask.com|    Kelwin|   Filshin|  35 Division Street|       Sweden| Gloves|   Large|  2|\n",
      "|01HPYDWM02SV85XYQ...|  7/2/2023|smalsher9@purevol...|      Shaw|   Malsher| 1907 Merchant Trail|    Argentina|Sweater|   Large|  4|\n",
      "|01HPYDWM0517RY5T1...|  1/1/2023|jrollinga@alibaba...|     Judie|   Rolling|        9 West Drive|      Ireland| Jacket|  Medium|  3|\n",
      "|01HPYDWM08SZEP369...|  5/9/2023| equyeb@mashable.com|    Emmett|      Quye|005 Kingsford Ter...|       France| Tshirt|XX-Large|  2|\n",
      "|01HPYDWM0AQ3D3RN2...| 3/30/2023|    mizzettc@irs.gov|    Mureil|    Izzett|  11 Myrtle Crossing|      Nigeria| Tshirt| X-Large|  4|\n",
      "|01HPYDWM0DQ02EJFN...|  6/5/2023|swolfendelld@wiki...|    Sileas|Wolfendell|       351 2nd Place|       Sweden|  Scarf|   Large|  2|\n",
      "|01HPYDWM0GB4B4HA0...| 12/5/2023|  ahillene@jimdo.com|     Aliza|    Hillen|7251 Independence...|       France|Sweater|  Medium|  4|\n",
      "|01HPYDWM0J83XQJPF...|  9/1/2023| sgethinsf@globo.com|    Sergio|   Gethins|    47959 Dayton Way|       Brazil| Tshirt|   Large|  2|\n",
      "|01HPYDWM0N5EVCWER...|12/10/2023|  fgarnamg@umich.edu|    Ferrel|    Garnam| 82839 Spaight Trail|United States| Tshirt| X-Large|  1|\n",
      "|01HPYDWM0RZ5HRTCN...| 6/16/2023|wmcsherryh@livejo...|      Waly|  McSherry|67348 Novick Parkway|        Chile|  Hoody|  Medium|  4|\n",
      "|01HPYDWM0V1N258VR...| 1/21/2023|kspellesyi@abc.ne...|    Katlin|  Spellesy| 44 Lotheville Trail|United States| Tshirt|  Medium|  2|\n",
      "|01HPYDWM0Y1ZQEVX0...| 7/11/2023|  hfurlongj@yelp.com|     Hilly|   Furlong|   984 Hoepker Place|       Brazil|Sweater|  Medium|  4|\n",
      "+--------------------+----------+--------------------+----------+----------+--------------------+-------------+-------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_dataset_from_github(url):\n",
    "    \"\"\"This function allows you to retrieve data from GitHub repositories.\n",
    "    \n",
    "    Input:\n",
    "    - url: String from the GitHub repository for the dataset in raw data\n",
    "    \n",
    "    Output:\n",
    "    - A dataset if successful, otherwise None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an exception if the status code is not 200 (OK)\n",
    "        dataset = pd.read_csv(url)\n",
    "        return dataset\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error downloading the dataset:', e)\n",
    "        return None\n",
    "#####\n",
    "    \n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GitHub Dataset\").getOrCreate()\n",
    "\n",
    "# Get the dataset from GitHub and convert it into a pandas DataFrame\n",
    "df_pd = get_dataset_from_github(url='https://raw.githubusercontent.com/jhnwr/auto-reporting/main/report1.csv')\n",
    "\n",
    "# Check if the DataFrame is not None\n",
    "if df_pd is not None:\n",
    "    # Convert pandas DataFrame to Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    df_spark.show()\n",
    "else:\n",
    "    print(\"Dataset could not be retrieved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
